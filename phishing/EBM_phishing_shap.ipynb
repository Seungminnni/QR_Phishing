{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a207e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) í™˜ê²½ ì„¤ì • ë° Import =====\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"í™˜ê²½ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b823a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1) ì„¤ì • =====\n",
    "DATA_PATH = \"/home/yu_mcc/QR_Phishing/phishing/phishing_data_tflite_ready.csv\"\n",
    "TARGET_COL = \"status\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"ì„¤ì • ì™„ë£Œ: RANDOM_SEED={RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce4ea3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2) ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ =====\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"ë°ì´í„° shape: {df.shape}\")\n",
    "print(f\"\\nì»¬ëŸ¼ ëª©ë¡:\\n{df.columns.tolist()}\")\n",
    "print(f\"\\nì²˜ìŒ 5í–‰:\\n{df.head()}\")\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸\n",
    "print(f\"\\níƒ€ê²Ÿ ë³€ìˆ˜ ({TARGET_COL}) ë¶„í¬:\")\n",
    "print(df[TARGET_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae0f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3) í”¼ì²˜ ë° ë ˆì´ë¸” ë¶„ë¦¬ (50:50 ê· í˜• ë¶„í• ) =====\n",
    "# ì›ë³¸ ë°ì´í„° ë³´ì¡´ì„ ìœ„í•´ ë³µì‚¬ë³¸ ì‚¬ìš©\n",
    "df_work = df.copy()\n",
    "\n",
    "# 'id' ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì œê±°\n",
    "if 'id' in df_work.columns:\n",
    "    df_work = df_work.drop(columns=['id'])\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "# status: 0 = ì •ìƒ, 1 = í”¼ì‹±\n",
    "print(f\"í”¼ì²˜ ê°œìˆ˜: {len(df_work.columns) - 1}\")\n",
    "print(f\"ì „ì²´ ë°ì´í„° shape: {df_work.shape}\")\n",
    "print(f\"\\nì›ë³¸ ë ˆì´ë¸” ë¶„í¬:\")\n",
    "print(f\"  - ì •ìƒ(0): {(df_work[TARGET_COL]==0).sum()}\")\n",
    "print(f\"  - í”¼ì‹±(1): {(df_work[TARGET_COL]==1).sum()}\")\n",
    "\n",
    "# ===== 50:50 ê· í˜• ë¶„í•  í•¨ìˆ˜ =====\n",
    "def split_class(class_df, train_ratio=0.6, val_ratio=0.2, seed=42):\n",
    "    \"\"\"í´ë˜ìŠ¤ë³„ë¡œ train/val/test ë¶„í• \"\"\"\n",
    "    # ì—¬ëŸ¬ ë²ˆ ì…”í”Œí•´ì„œ íŒ¨í„´ í¸í–¥ ë°©ì§€\n",
    "    shuffled = class_df.sample(frac=1, random_state=seed)\n",
    "    shuffled = shuffled.sample(frac=1, random_state=seed+1)\n",
    "    shuffled = shuffled.sample(frac=1, random_state=seed+2)\n",
    "    \n",
    "    n = len(shuffled)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    return (shuffled.iloc[:train_end], \n",
    "            shuffled.iloc[train_end:val_end], \n",
    "            shuffled.iloc[val_end:])\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ë¡œ ë¶„ë¦¬\n",
    "df_legit = df_work[df_work[TARGET_COL] == 0]  # ì •ìƒ\n",
    "df_phish = df_work[df_work[TARGET_COL] == 1]  # í”¼ì‹±\n",
    "\n",
    "# ê° í´ë˜ìŠ¤ë³„ë¡œ 60/20/20 ë¶„í• \n",
    "legit_train, legit_val, legit_test = split_class(df_legit, seed=RANDOM_SEED)\n",
    "phish_train, phish_val, phish_test = split_class(df_phish, seed=RANDOM_SEED)\n",
    "\n",
    "# í•©ì¹˜ê¸° (ê° ì„¸íŠ¸ì—ì„œ 50:50 ë¹„ìœ¨ ìœ ì§€)\n",
    "train_df = pd.concat([legit_train, phish_train]).sample(frac=1, random_state=RANDOM_SEED)\n",
    "val_df = pd.concat([legit_val, phish_val]).sample(frac=1, random_state=RANDOM_SEED)\n",
    "test_df = pd.concat([legit_test, phish_test]).sample(frac=1, random_state=RANDOM_SEED)\n",
    "\n",
    "# í”¼ì²˜ì™€ ë ˆì´ë¸” ë¶„ë¦¬\n",
    "X_train = train_df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
    "y_train = train_df[TARGET_COL].values.astype(np.float32)\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
    "y_val = val_df[TARGET_COL].values.astype(np.float32)\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
    "y_test = test_df[TARGET_COL].values.astype(np.float32)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"\\n===== 50:50 ê· í˜• ë¶„í•  ê²°ê³¼ =====\")\n",
    "print(f\"Train: {X_train.shape} - ì •ìƒ:{(y_train==0).sum()}, í”¼ì‹±:{(y_train==1).sum()}\")\n",
    "print(f\"Val:   {X_val.shape} - ì •ìƒ:{(y_val==0).sum()}, í”¼ì‹±:{(y_val==1).sum()}\")\n",
    "print(f\"Test:  {X_test.shape} - ì •ìƒ:{(y_test==0).sum()}, í”¼ì‹±:{(y_test==1).sum()}\")\n",
    "\n",
    "# ë¹„ìœ¨ í™•ì¸\n",
    "print(f\"\\n===== ê° ì„¸íŠ¸ì˜ í”¼ì‹± ë¹„ìœ¨ =====\")\n",
    "print(f\"Train í”¼ì‹± ë¹„ìœ¨: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"Val í”¼ì‹± ë¹„ìœ¨:   {y_val.mean()*100:.1f}%\")\n",
    "print(f\"Test í”¼ì‹± ë¹„ìœ¨:  {y_test.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d74a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4) ë°ì´í„° ì „ì²˜ë¦¬ (RobustScaler - embedding_modelê³¼ ë™ì¼) =====\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# RobustScalerë¥¼ ì ìš©í•  í”¼ì²˜ ëª©ë¡ (ì´ìƒì¹˜ê°€ ë§ì€ í”¼ì²˜ë“¤)\n",
    "robust_cols = [\n",
    "    'length_url',\n",
    "    'length_hostname',\n",
    "    'nb_dots',\n",
    "    'nb_hyphens',\n",
    "    'nb_and',\n",
    "    'nb_eq',\n",
    "    'nb_underscore',\n",
    "    'nb_percent',\n",
    "    'nb_slash',\n",
    "    'nb_colon',\n",
    "    'nb_semicolumn',\n",
    "    'nb_space',\n",
    "    'nb_com',\n",
    "    'ratio_digits_url',\n",
    "    'ratio_digits_host',\n",
    "    'length_words_raw',\n",
    "    'char_repeat',\n",
    "    'shortest_words_raw',\n",
    "    'shortest_word_host',\n",
    "    'shortest_word_path',\n",
    "    'longest_words_raw',\n",
    "    'longest_word_host',\n",
    "    'longest_word_path',\n",
    "    'avg_words_raw',\n",
    "    'avg_word_host',\n",
    "    'avg_word_path',\n",
    "    'phish_hints',\n",
    "    'nb_extCSS',\n",
    "]\n",
    "\n",
    "# í”¼ì²˜ ì¸ë±ìŠ¤ ë§¤í•‘\n",
    "feature_names = [col for col in df_work.columns if col != TARGET_COL]\n",
    "robust_indices = [feature_names.index(col) for col in robust_cols if col in feature_names]\n",
    "raw_indices = [i for i in range(len(feature_names)) if i not in robust_indices]\n",
    "\n",
    "print(f\"RobustScaler ì ìš© í”¼ì²˜: {len(robust_indices)}ê°œ\")\n",
    "print(f\"ì •ê·œí™” ì—†ìŒ (ì›ë³¸ ê·¸ëŒ€ë¡œ): {len(raw_indices)}ê°œ\")\n",
    "\n",
    "# RobustScaler ìƒì„±\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰\n",
    "X_train_scaled = X_train.copy().astype(np.float32)\n",
    "X_val_scaled = X_val.copy().astype(np.float32)\n",
    "X_test_scaled = X_test.copy().astype(np.float32)\n",
    "\n",
    "# RobustScaler ì ìš©ë§Œ (ì¤‘ì•™ê°’ ê¸°ë°˜ - ì´ìƒì¹˜ ì˜í–¥ ì ìŒ)\n",
    "X_train_scaled[:, robust_indices] = robust_scaler.fit_transform(X_train[:, robust_indices]).astype(np.float32)\n",
    "X_val_scaled[:, robust_indices] = robust_scaler.transform(X_val[:, robust_indices]).astype(np.float32)\n",
    "X_test_scaled[:, robust_indices] = robust_scaler.transform(X_test[:, robust_indices]).astype(np.float32)\n",
    "\n",
    "# raw_indicesëŠ” ê·¸ëŒ€ë¡œ ë‘ê¸° (ì •ê·œí™” ì•ˆ í•¨)\n",
    "\n",
    "print(\"\\në°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ (RobustScaler - embedding_modelê³¼ ë™ì¼)\")\n",
    "print(f\"Train í”¼ì²˜ ë²”ìœ„: min={X_train_scaled.min():.2f}, max={X_train_scaled.max():.2f}\")\n",
    "print(f\"Train í”¼ì²˜ í‰ê· : {X_train_scaled.mean():.4f}, í‘œì¤€í¸ì°¨: {X_train_scaled.std():.4f}\")\n",
    "print(f\"í”¼ì²˜ ê°œìˆ˜: {X_train_scaled.shape[1]}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959446c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 5) EBM ëª¨ë¸ í•™ìŠµ =====\n",
    "print(\"EBM(Explainable Boosting Machine) ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "\n",
    "ebm_model = ExplainableBoostingClassifier(\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_bins=256,\n",
    "    max_interaction_bins=32,\n",
    "    interactions=10,\n",
    "    outer_bags=8,\n",
    "    inner_bags=0,\n",
    "    learning_rate=0.05,\n",
    "    validation_size=0.0\n",
    ")\n",
    "\n",
    "# í•™ìŠµ\n",
    "ebm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"âœ… EBM ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"   í”¼ì²˜ ê°œìˆ˜: {len(feature_names)}\")\n",
    "print(f\"   í´ë˜ìŠ¤: {ebm_model.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a915e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ë¶„í•  ê²€ì¦ (ë°ì´í„° ì†ì‹¤/ì¤‘ë³µ í™•ì¸) =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ë¶„í•  ê²€ì¦\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1) ë°ì´í„° ì¤‘ë³µ/ì†ì‹¤ í™•ì¸\n",
    "total_samples = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"\\n1ï¸âƒ£ ë°ì´í„° ë¬´ê²°ì„± í™•ì¸:\")\n",
    "print(f\"   Train + Val + Test = {total_samples}\")\n",
    "print(f\"   ì›ë³¸ ë°ì´í„° = {len(X_train_scaled) + len(X_val_scaled) + len(X_test_scaled)}\")\n",
    "print(f\"   âœ… ì¼ì¹˜\" if total_samples == len(df_work) else f\"   âŒ ë¶ˆì¼ì¹˜\")\n",
    "\n",
    "# 2) ë¹„ìœ¨ í™•ì¸\n",
    "print(f\"\\n2ï¸âƒ£ ë¶„í•  ë¹„ìœ¨ í™•ì¸ (60:20:20):\")\n",
    "print(f\"   Train: {len(X_train)} ({len(X_train)/len(df_work)*100:.1f}%)\")\n",
    "print(f\"   Val:   {len(X_val)} ({len(X_val)/len(df_work)*100:.1f}%)\")\n",
    "print(f\"   Test:  {len(X_test)} ({len(X_test)/len(df_work)*100:.1f}%)\")\n",
    "\n",
    "# 3) í´ë˜ìŠ¤ ê· í˜• í™•ì¸\n",
    "print(f\"\\n3ï¸âƒ£ í´ë˜ìŠ¤ ê· í˜• í™•ì¸ (ê° ì„¸íŠ¸ 50:50):\")\n",
    "for name, y in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
    "    legit_count = (y==0).sum()\n",
    "    phish_count = (y==1).sum()\n",
    "    legit_ratio = legit_count / len(y) * 100\n",
    "    phish_ratio = phish_count / len(y) * 100\n",
    "    print(f\"   {name}: ì •ìƒ {legit_count}({legit_ratio:.1f}%), í”¼ì‹± {phish_count}({phish_ratio:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7aa9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 6) ëª¨ë¸ í‰ê°€ =====\n",
    "print(\"=\"*60)\n",
    "print(\"ëª¨ë¸ í‰ê°€ (Test Set)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "y_pred = ebm_model.predict(X_test_scaled)\n",
    "y_pred_proba = ebm_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# ë ˆì´ë¸”ì„ ì •ìˆ˜ë¡œ ë³€í™˜ (íƒ€ì… ì¼ì¹˜)\n",
    "y_test_int = y_test.astype(int)\n",
    "y_pred_int = np.array(y_pred, dtype=float).astype(int)\n",
    "\n",
    "# ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "accuracy = accuracy_score(y_test_int, y_pred_int)\n",
    "precision = precision_score(y_test_int, y_pred_int)\n",
    "recall = recall_score(y_test_int, y_pred_int)\n",
    "f1 = f1_score(y_test_int, y_pred_int)\n",
    "auc_roc = roc_auc_score(y_test_int, y_pred_proba)\n",
    "\n",
    "print(f\"\\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:\")\n",
    "print(f\"  - Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  - Precision: {precision:.4f}\")\n",
    "print(f\"  - Recall:    {recall:.4f}\")\n",
    "print(f\"  - F1 Score:  {f1:.4f}\")\n",
    "print(f\"  - AUC-ROC:   {auc_roc:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_test_int, y_pred_int, target_names=['Legitimate', 'Phishing']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "974814af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 7) ì‹œê°í™” (Confusion Matrix, ROC Curve) =====\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_int, y_pred_int)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Legitimate', 'Phishing'],\n",
    "            yticklabels=['Legitimate', 'Phishing'])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test_int, y_pred_proba)\n",
    "axes[1].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_roc:.4f})', linewidth=2)\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b8edc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 8) ìµœì  ì„ê³„ê°’ ë¶„ì„ =====\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ ìµœì  ì„ê³„ê°’ ë¶„ì„\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œ F1 Score ê³„ì‚° (0.05 ~ 0.95)\n",
    "thresholds_to_test = np.arange(0.05, 1.0, 0.05)\n",
    "f1_scores = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    pred_binary = (y_pred_proba > thresh).astype(int)\n",
    "    f1 = f1_score(y_test_int, pred_binary)\n",
    "    acc = accuracy_score(y_test_int, pred_binary)\n",
    "    prec = precision_score(y_test_int, pred_binary, zero_division=0)\n",
    "    rec = recall_score(y_test_int, pred_binary, zero_division=0)\n",
    "    f1_scores.append(f1)\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "# ìµœì  ì„ê³„ê°’ ì°¾ê¸° (F1 ê¸°ì¤€)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds_to_test[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "# ê¸°ë³¸ 0.5 ì„±ëŠ¥\n",
    "default_f1 = f1_score(y_test_int, (y_pred_proba > 0.5).astype(int))\n",
    "default_acc = accuracy_score(y_test_int, (y_pred_proba > 0.5).astype(int))\n",
    "\n",
    "print(f\"\\nğŸ“Š ì„ê³„ê°’ë³„ F1 Score (ìƒìœ„ 10ê°œ):\")\n",
    "sorted_indices = np.argsort(f1_scores)[::-1][:10]\n",
    "for idx in sorted_indices:\n",
    "    thresh = thresholds_to_test[idx]\n",
    "    f1 = f1_scores[idx]\n",
    "    marker = \" â­ BEST\" if idx == best_idx else \"\"\n",
    "    default_marker = \" (ê¸°ë³¸ê°’)\" if abs(thresh - 0.5) < 0.01 else \"\"\n",
    "    print(f\"   Threshold {thresh:.2f}: F1 = {f1:.4f}{marker}{default_marker}\")\n",
    "\n",
    "print(f\"\\nâœ… ìµœì  ì„ê³„ê°’: {best_threshold:.2f}\")\n",
    "print(f\"   - F1 Score: {best_f1:.4f}\")\n",
    "print(f\"   - Accuracy: {accuracies[best_idx]:.4f}\")\n",
    "print(f\"   - Precision: {precisions[best_idx]:.4f}\")\n",
    "print(f\"   - Recall: {recalls[best_idx]:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Œ ê¸°ë³¸ ì„ê³„ê°’ 0.5 ì„±ëŠ¥:\")\n",
    "print(f\"   - F1 Score: {default_f1:.4f}\")\n",
    "print(f\"   - Accuracy: {default_acc:.4f}\")\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 vs Threshold\n",
    "axes[0].plot(thresholds_to_test, f1_scores, 'b-', linewidth=2, label='F1 Score')\n",
    "axes[0].plot(thresholds_to_test, accuracies, 'g--', linewidth=2, label='Accuracy')\n",
    "axes[0].axvline(x=best_threshold, color='red', linestyle='--', linewidth=2, label=f'Best: {best_threshold:.2f}')\n",
    "axes[0].axvline(x=0.5, color='orange', linestyle=':', linewidth=2, label='Default: 0.5')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('F1 Score & Accuracy vs Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ (í´ë˜ìŠ¤ë³„) + ì„ê³„ê°’ í‘œì‹œ\n",
    "axes[1].hist(y_pred_proba[y_test_int==0], bins=30, alpha=0.6, label='Legitimate (0)', color='blue')\n",
    "axes[1].hist(y_pred_proba[y_test_int==1], bins=30, alpha=0.6, label='Phishing (1)', color='red')\n",
    "axes[1].axvline(x=best_threshold, color='red', linestyle='--', linewidth=2, label=f'Best: {best_threshold:.2f}')\n",
    "axes[1].axvline(x=0.5, color='orange', linestyle=':', linewidth=2, label='Default: 0.5')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Prediction Distribution by Class')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ê²°ë¡ \n",
    "diff = abs(best_threshold - 0.5)\n",
    "f1_improvement = best_f1 - default_f1\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ ê²°ë¡ \")\n",
    "print(\"=\"*60)\n",
    "if diff < 0.1 and f1_improvement < 0.01:\n",
    "    print(f\"âœ… 0.5ì™€ ìµœì  ì„ê³„ê°’ ì°¨ì´ê°€ {diff:.2f}ë¡œ ì‘ê³ , F1 ê°œì„ ë„ {f1_improvement:.4f}ë¡œ ë¯¸ë¯¸í•©ë‹ˆë‹¤.\")\n",
    "    print(f\"   â†’ ê¸°ë³¸ê°’ 0.5 ì‚¬ìš©í•´ë„ ë©ë‹ˆë‹¤.\")\n",
    "    FINAL_THRESHOLD = 0.5\n",
    "else:\n",
    "    print(f\"âš ï¸ ìµœì  ì„ê³„ê°’({best_threshold:.2f})ì´ 0.5ì™€ {diff:.2f} ì°¨ì´ë‚©ë‹ˆë‹¤.\")\n",
    "    print(f\"   F1 ê°œì„ : {default_f1:.4f} â†’ {best_f1:.4f} (+{f1_improvement:.4f})\")\n",
    "    print(f\"   â†’ Android ì•±ì—ì„œ {best_threshold:.2f}ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\")\n",
    "    FINAL_THRESHOLD = best_threshold\n",
    "\n",
    "print(f\"\\nğŸ¯ ìµœì¢… ì‚¬ìš© ì„ê³„ê°’: {FINAL_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df1a5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 9) SHAP ë¶„ì„ - EBM ëª¨ë¸ìš© =====\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shap\n",
    "\n",
    "OUTPUT_DIR = \"/home/yu_mcc/QR_Phishing/phishing\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SHAP ë¶„ì„ ì‹œì‘ (EBM ëª¨ë¸)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ë°ì´í„°ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜ (feature names í¬í•¨)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "X_background_df = pd.DataFrame(X_train_scaled[:100], columns=feature_names)  # ë°°ê²½ ë°ì´í„°\n",
    "\n",
    "# KernelExplainer ìƒì„± (predict_proba ì‚¬ìš©)\n",
    "explainer = shap.KernelExplainer(\n",
    "    lambda x: ebm_model.predict_proba(x)[:, 1],\n",
    "    X_background_df\n",
    ")\n",
    "\n",
    "# ìƒ˜í”Œë§ (ê³„ì‚° ì†ë„)\n",
    "sample_size = min(200, len(X_test_df))\n",
    "sample_indices = np.random.choice(len(X_test_df), sample_size, replace=False)\n",
    "X_test_sample_df = X_test_df.iloc[sample_indices]\n",
    "\n",
    "print(f\"ë°°ê²½ ë°ì´í„°: {X_background_df.shape}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test_sample_df.shape}\")\n",
    "\n",
    "# SHAP ê°’ ê³„ì‚°\n",
    "shap_values = explainer.shap_values(X_test_sample_df)\n",
    "\n",
    "print(f\"âœ… SHAP ê³„ì‚° ì™„ë£Œ!\")\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"Feature names count: {len(feature_names)}\")\n",
    "\n",
    "# SHAP ê°’ì˜ í‰ê·  ê³„ì‚°\n",
    "mean_shap_signed = np.mean(shap_values, axis=0)\n",
    "mean_abs_shap = np.abs(mean_shap_signed)\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„± (ì ˆëŒ€ê°’ ê¸°ì¤€ ì •ë ¬)\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap_signed': mean_shap_signed,\n",
    "    'shap_abs': mean_abs_shap\n",
    "}).sort_values('shap_abs', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 54ê°œë¥¼ 27ê°œì”© ë‚˜ëˆ„ê¸°\n",
    "n_features = len(importance_df)\n",
    "half = n_features // 2  # 27ê°œ\n",
    "\n",
    "top_half = importance_df.iloc[:half].sort_values('shap_abs', ascending=True)  # ìƒìœ„ 27ê°œ\n",
    "bottom_half = importance_df.iloc[half:].sort_values('shap_abs', ascending=True)  # í•˜ìœ„ 27ê°œ\n",
    "\n",
    "# íˆ¬ì»¬ëŸ¼ ê·¸ë˜í”„ ìƒì„±\n",
    "fig, axes = plt.subplots(1, 2, figsize=(26, 14))\n",
    "\n",
    "for ax, data, title in zip(axes, [top_half, bottom_half], \n",
    "                            ['Top 27 Features (Rank 1-27)', 'Bottom 27 Features (Rank 28-54)']):\n",
    "    colors = ['#1f77b4' if x < 0 else '#d62728' for x in data['shap_signed']]\n",
    "    \n",
    "    ax.barh(range(len(data)), data['shap_signed'], color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=1.5)\n",
    "    \n",
    "    ax.set_yticks(range(len(data)))\n",
    "    ax.set_yticklabels(data['feature'], fontsize=9)\n",
    "    ax.set_xlabel('Mean SHAP value', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # ê°’ í‘œì‹œ (offsetì„ ë™ì ìœ¼ë¡œ ê³„ì‚°)\n",
    "    x_range = data['shap_abs'].max()\n",
    "    offset = x_range * 0.01  # ë§‰ëŒ€ ìµœëŒ€ê°’ì˜ 1%ë§Œí¼ offset\n",
    "    \n",
    "    for i, (feat, val_signed, val_abs) in enumerate(zip(data['feature'], data['shap_signed'], data['shap_abs'])):\n",
    "        sign = '+' if val_signed > 0 else ''\n",
    "        if val_signed < 0:\n",
    "            ax.text(val_signed - offset, i, f'{val_signed:.4f}', va='center', ha='right', fontsize=7)\n",
    "        else:\n",
    "            ax.text(val_signed + offset, i, f'{sign}{val_signed:.4f}', va='center', ha='left', fontsize=7)\n",
    "    \n",
    "    # ë²”ë¡€ (ê° ì„œë¸Œí”Œë¡¯ì— ë°°ì¹˜)\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#1f77b4', alpha=0.7, label='Negative: Legitimate'),\n",
    "        Patch(facecolor='#d62728', alpha=0.7, label='Positive: Phishing')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
    "\n",
    "plt.suptitle('SHAP Feature Importance with Direction (All 54 Features)', fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'shap_bar_twocolumn.png'), dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… Saved: {os.path.join(OUTPUT_DIR, 'shap_bar_twocolumn.png')}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š í”¼ì²˜ ìˆœìœ„ (ì ˆëŒ€ê°’ ê¸°ì¤€):\")\n",
    "print(\"-\"*60)\n",
    "for rank, (_, row) in enumerate(importance_df.iterrows(), 1):\n",
    "    direction = 'â†’ Phishing' if row['shap_signed'] > 0 else 'â† Legitimate'\n",
    "    print(f\"{rank:>2}. {row['feature']:<25} {row['shap_signed']:>10.6f} {direction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e2f1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 10) SHAP Summary Plot (ë…¼ë¬¸ìš© íˆ¬ì»¬ëŸ¼) =====\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"/home/yu_mcc/QR_Phishing/phishing\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SHAP Summary Plot (54 Features - ë…¼ë¬¸ìš© íˆ¬ì»¬ëŸ¼)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "\n",
    "# í”¼ì²˜ ì¤‘ìš”ë„ ìˆœì„œë¡œ ì •ë ¬\n",
    "mean_abs_shap = np.mean(np.abs(shap_values), axis=0).flatten()\n",
    "sorted_idx = np.argsort(mean_abs_shap)[::-1]  # ì¤‘ìš”ë„ ë†’ì€ ìˆœ\n",
    "\n",
    "# 54ê°œë¥¼ 27ê°œì”© ë‚˜ëˆ„ê¸°\n",
    "n_features = len(feature_names)\n",
    "half = n_features // 2  # 27ê°œ\n",
    "\n",
    "top_idx = sorted_idx[:half]  # ìƒìœ„ 27ê°œ\n",
    "bottom_idx = sorted_idx[half:]  # í•˜ìœ„ 27ê°œ\n",
    "\n",
    "# íˆ¬ì»¬ëŸ¼ ê·¸ë˜í”„ ìƒì„±\n",
    "fig, axes = plt.subplots(1, 2, figsize=(26, 14))\n",
    "\n",
    "for ax, indices, title in zip(axes, [top_idx, bottom_idx], \n",
    "                               ['Top 27 Features (Rank 1-27)', 'Bottom 27 Features (Rank 28-54)']):\n",
    "    # í•´ë‹¹ í”¼ì²˜ë§Œ ì¶”ì¶œ\n",
    "    shap_subset = shap_values[:, indices]\n",
    "    X_subset = X_test_sample_df.iloc[:, indices]\n",
    "    feature_subset = [feature_names[i] for i in indices]\n",
    "    \n",
    "    plt.sca(ax)\n",
    "    shap.summary_plot(\n",
    "        shap_subset, \n",
    "        X_subset, \n",
    "        feature_names=feature_subset,\n",
    "        max_display=27,\n",
    "        show=False,\n",
    "        plot_size=None\n",
    "    )\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('SHAP Summary Plot (All 54 Features)', fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# ì €ì¥\n",
    "dot_plot_path = os.path.join(OUTPUT_DIR, 'shap_summary_twocolumn.png')\n",
    "plt.savefig(dot_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… Saved: {dot_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Top 10 í”¼ì²˜ ì¶œë ¥\n",
    "print(f\"\\nğŸ“Š Top 10 ì¤‘ìš” í”¼ì²˜:\")\n",
    "for i, idx in enumerate(sorted_idx[:10]):\n",
    "    print(f\"  {i+1}. {feature_names[idx]}: {mean_abs_shap[idx]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š í•´ì„ ê°€ì´ë“œ:\")\n",
    "print(\"=\"*70)\n",
    "print(\"  - ë¹¨ê°„ì (ë†’ì€ í”¼ì²˜ ê°’)ì´ ì˜¤ë¥¸ìª½(+)ì— ìœ„ì¹˜ â†’ ë†’ì€ ê°’ì´ í”¼ì‹± ì˜ˆì¸¡ ì¦ê°€\")\n",
    "print(\"  - íŒŒë€ì (ë‚®ì€ í”¼ì²˜ ê°’)ì´ ì™¼ìª½(-)ì— ìœ„ì¹˜ â†’ ë‚®ì€ ê°’ì´ ì •ìƒ ì˜ˆì¸¡ ì¦ê°€\")\n",
    "print(\"  - ì ë“¤ì˜ ë°€ë„ê°€ ë†’ì„ìˆ˜ë¡ í•´ë‹¹ SHAP ê°’ì— ë§ì€ ìƒ˜í”Œì´ ë¶„í¬\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}