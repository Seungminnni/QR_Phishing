{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ - 19ê°œ URL íŠ¹ìˆ˜ë¬¸ì í”¼ì²˜\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ embedding_modelì˜ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ 19ê°œì˜ URL íŠ¹ìˆ˜ë¬¸ì í”¼ì²˜ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©í‘œ\n",
    "- Autoencoderë¥¼ ì‚¬ìš©í•˜ì—¬ 19ê°œ í”¼ì²˜ì˜ ì €ì°¨ì› ì„ë² ë”© ìƒì„±\n",
    "- Dataset1ê³¼ Dataset2ë¥¼ í†µí•©í•œ ê· í˜•ì¡íŒ ë°ì´í„°ë¡œ í•™ìŠµ\n",
    "- í•™ìŠµëœ ì„ë² ë”© ê³µê°„ì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œ í”¼ì‹± íƒì§€\n",
    "- ì •ìƒ/í”¼ì‹± ìƒ˜í”Œì˜ í”„ë¡œí† íƒ€ì… ë²¡í„° ìƒì„± ë° ì €ì¥\n",
    "\n",
    "## í”„ë ˆì„ì›Œí¬\n",
    "1. í™˜ê²½ ì´ˆê¸°í™” (GPU ì„¤ì •)\n",
    "2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "3. Autoencoder ëª¨ë¸ í•™ìŠµ\n",
    "4. ì„ë² ë”© ê³µê°„ì—ì„œ ì •ìƒ/í”¼ì‹± í”„ë¡œí† íƒ€ì… ìƒì„±\n",
    "5. ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€\n",
    "6. ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 0) ëª¨ë¸ ì´ˆê¸°í™” (Linux NVIDIA GPU) ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "\n",
    "# NVIDIA GPU í™•ì¸ ë° í™œì„±í™”\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"âœ… NVIDIA GPU ë°œê²¬: {len(gpus)}ê°œ\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"   - {gpu.name}\")\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"âœ… GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ í™œì„±í™”ë¨!\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ ë©”ëª¨ë¦¬ ì„¤ì • ì‹¤íŒ¨: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ GPUê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# GPU ì—°ì‚° í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n=== GPU ì—°ì‚° í…ŒìŠ¤íŠ¸ ===\")\n",
    "try:\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.random.normal([1000, 1000])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(f\"âœ… GPU ì—°ì‚° í…ŒìŠ¤íŠ¸ ì™„ë£Œ: shape = {c.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ GPU ì—°ì‚° ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}\")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "print(\"\\nâœ… TensorFlow ë° GPU ì´ˆê¸°í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 1) í™˜ê²½ ì„¤ì • ë° Import ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")\n",
    "print(f\"Pandas ë²„ì „: {pd.__version__}\")\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"Scikit-learn ë²„ì „: {__import__('sklearn').__version__}\")\n",
    "print(\"\\nâœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ Import ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 2) ì„¤ì • ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Linux í™˜ê²½ ê²½ë¡œ\n",
    "DATASET_DIR = '/home/wza/QR_Phishing/phishing/Deviation-in-Feature-Contribution-7760/Dataset'\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°\n",
    "DATA1_TRAIN = os.path.join(DATASET_DIR, 'original_training_dataset1_70_30.csv')\n",
    "DATA2_TRAIN = os.path.join(DATASET_DIR, 'Training/Dataset2/70_30_train.csv')\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "DATA1_TEST = os.path.join(DATASET_DIR, 'Testing/Dataset1/70_30_test.csv')\n",
    "DATA2_TEST = os.path.join(DATASET_DIR, 'Testing/Dataset2/70_30_test.csv')\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "EMBEDDING_DIM = 32\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "RANDOM_SEED = 42\n",
    "L2_REG = 0.0001\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# 19ê°œ í”¼ì²˜ (url_google_index ì œì™¸)\n",
    "FEATURE_NAMES = [\n",
    "    'qty_dot_url', 'qty_hyphen_url', 'qty_underline_url', 'qty_slash_url',\n",
    "    'qty_questionmark_url', 'qty_equal_url', 'qty_at_url', 'qty_and_url',\n",
    "    'qty_tilde_url', 'qty_comma_url', 'qty_asterisk_url', 'qty_percent_url',\n",
    "    'qty_dollar_url', 'tld_present_params', 'qty_redirects', 'length_url',\n",
    "    'url_shortened', 'domain_length', 'domain_in_ip'\n",
    "]\n",
    "\n",
    "# Dataset2 ì»¬ëŸ¼ ë§¤í•‘\n",
    "DATASET2_COLS = {\n",
    "    'total_of.': 'qty_dot_url',\n",
    "    'total_of-': 'qty_hyphen_url',\n",
    "    'total_of_': 'qty_underline_url',\n",
    "    'total_of/': 'qty_slash_url',\n",
    "    'total_of?': 'qty_questionmark_url',\n",
    "    'total_of=': 'qty_equal_url',\n",
    "    'total_of@': 'qty_at_url',\n",
    "    'total_of&': 'qty_and_url',\n",
    "    'total_of~': 'qty_tilde_url',\n",
    "    'total_of,': 'qty_comma_url',\n",
    "    'total_of*': 'qty_asterisk_url',\n",
    "    'total_of%': 'qty_percent_url',\n",
    "    'total_of$': 'qty_dollar_url',\n",
    "    'tld_in_path': 'tld_present_params',\n",
    "    'nb_redirection': 'qty_redirects',\n",
    "    'url_length': 'length_url',\n",
    "    'shortening_service': 'url_shortened',\n",
    "    'hostname_length': 'domain_length',\n",
    "    'ip': 'domain_in_ip',\n",
    "    'status': 'phishing'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”§ ì„¤ì •\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Random Seed: {RANDOM_SEED}\")\n",
    "print(f\"í”¼ì²˜ ê°œìˆ˜: {len(FEATURE_NAMES)}\")\n",
    "print(f\"ì„ë² ë”© ì°¨ì›: {EMBEDDING_DIM}\")\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}, ì—í­: {EPOCHS}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}, L2 Regularization: {L2_REG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 3) ë°ì´í„° ë¡œë“œ ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° ë¡œë“œ\n",
    "df1_train = pd.read_csv(DATA1_TRAIN, encoding='utf-8', engine='python')\n",
    "df2_train = pd.read_csv(DATA2_TRAIN, encoding='utf-8', engine='python')\n",
    "\n",
    "print(f\"\\nâœ… í•™ìŠµ ë°ì´í„° ë¡œë“œ ì™„ë£Œ:\")\n",
    "print(f\"   - Dataset1 Train: {df1_train.shape}\")\n",
    "print(f\"   - Dataset2 Train: {df2_train.shape}\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ë¶„í¬ (D1): {df1_train['phishing'].value_counts().to_dict()}\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ë¶„í¬ (D2): {df2_train['status'].value_counts().to_dict()}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "df1_test = pd.read_csv(DATA1_TEST, encoding='utf-8', engine='python')\n",
    "df2_test = pd.read_csv(DATA2_TEST, encoding='utf-8', engine='python')\n",
    "\n",
    "print(f\"\\nâœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ:\")\n",
    "print(f\"   - Dataset1 Test: {df1_test.shape}\")\n",
    "print(f\"   - Dataset2 Test: {df2_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 4) ë°ì´í„° ì „ì²˜ë¦¬ (19ê°œ í”¼ì²˜ë§Œ ì„ íƒ, 50:50 ê· í˜• ë¶„í• ) ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”„ í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dataset1ì—ì„œ 19ê°œ í”¼ì²˜ ì„ íƒ\n",
    "X1_train = df1_train[FEATURE_NAMES].values.astype(np.float32)\n",
    "y1_train = df1_train['phishing'].values.astype(np.float32)\n",
    "\n",
    "print(f\"\\nâœ… Dataset1 í•™ìŠµ ë°ì´í„°:\")\n",
    "print(f\"   - X shape: {X1_train.shape}\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y1_train.astype(int))}\")\n",
    "\n",
    "# Dataset2 ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½\n",
    "df2_train_renamed = df2_train.rename(columns=DATASET2_COLS)\n",
    "X2_train = df2_train_renamed[FEATURE_NAMES].values.astype(np.float32)\n",
    "y2_train = df2_train_renamed['phishing'].values.astype(np.float32)\n",
    "\n",
    "print(f\"\\nâœ… Dataset2 í•™ìŠµ ë°ì´í„°:\")\n",
    "print(f\"   - X shape: {X2_train.shape}\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y2_train.astype(int))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 50:50 ê· í˜• ë¶„í•  í•¨ìˆ˜ =====\n",
    "def split_class(X, y, train_ratio=0.6, val_ratio=0.2, seed=42):\n",
    "    \"\"\"í´ë˜ìŠ¤ë³„ë¡œ train/val/test ë¶„í• \"\"\"\n",
    "    # ì—¬ëŸ¬ ë²ˆ ì…”í”Œí•´ì„œ íŒ¨í„´ í¸í–¥ ë°©ì§€\n",
    "    indices = np.arange(len(X))\n",
    "    for i in range(3):\n",
    "        np.random.seed(seed + i)\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    X_shuffled = X[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    n = len(X_shuffled)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    return (X_shuffled[:train_end], y_shuffled[:train_end],\n",
    "            X_shuffled[train_end:val_end], y_shuffled[train_end:val_end],\n",
    "            X_shuffled[val_end:], y_shuffled[val_end:])\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ë¡œ ë¶„ë¦¬\n",
    "X1_phish = X1_train[y1_train == 1]\n",
    "y1_phish = y1_train[y1_train == 1]\n",
    "X1_legit = X1_train[y1_train == 0]\n",
    "y1_legit = y1_train[y1_train == 0]\n",
    "\n",
    "# ê° í´ë˜ìŠ¤ë³„ë¡œ 60/20/20 ë¶„í• \n",
    "X1_phish_tr, y1_phish_tr, X1_phish_val, y1_phish_val, X1_phish_te, y1_phish_te = split_class(X1_phish, y1_phish, seed=RANDOM_SEED)\n",
    "X1_legit_tr, y1_legit_tr, X1_legit_val, y1_legit_val, X1_legit_te, y1_legit_te = split_class(X1_legit, y1_legit, seed=RANDOM_SEED)\n",
    "\n",
    "# ê· í˜•ì¡íŒ D1 + D2\n",
    "X_merged_train = np.vstack([X1_phish_tr, X1_legit_tr, X2_train])\n",
    "y_merged_train = np.hstack([y1_phish_tr, y1_legit_tr, y2_train])\n",
    "\n",
    "# ì…”í”Œ\n",
    "indices = np.arange(len(X_merged_train))\n",
    "np.random.shuffle(indices)\n",
    "X_merged_train = X_merged_train[indices]\n",
    "y_merged_train = y_merged_train[indices]\n",
    "\n",
    "print(f\"\\nâœ… Dataset1 ê· í˜• ì¡°ì • (60/20/20 ë¶„í• ):\")\n",
    "print(f\"   - Train: í”¼ì‹±{int((y1_phish_tr==1).sum())}, ì •ìƒ{int((y1_legit_tr==0).sum())}\")\n",
    "print(f\"   - Val: í”¼ì‹±{int((y1_phish_val==1).sum())}, ì •ìƒ{int((y1_legit_val==0).sum())}\")\n",
    "print(f\"   - Test: í”¼ì‹±{int((y1_phish_te==1).sum())}, ì •ìƒ{int((y1_legit_te==0).sum())}\")\n",
    "\n",
    "print(f\"\\nâœ… í†µí•© í•™ìŠµ ë°ì´í„°:\")\n",
    "print(f\"   - X shape: {X_merged_train.shape}\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_merged_train.astype(int))}\")\n",
    "print(f\"   - í”¼ì‹± ë¹„ìœ¨: {y_merged_train.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 5) ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”„ ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dataset1 ê²€ì¦/í…ŒìŠ¤íŠ¸\n",
    "X_val = np.vstack([X1_phish_val, X1_legit_val])\n",
    "y_val = np.hstack([y1_phish_val, y1_legit_val])\n",
    "\n",
    "X1_test_arr = df1_test[FEATURE_NAMES].values.astype(np.float32)\n",
    "y1_test_arr = df1_test['phishing'].values.astype(np.float32)\n",
    "\n",
    "# Dataset2 í…ŒìŠ¤íŠ¸\n",
    "df2_test_renamed = df2_test.rename(columns=DATASET2_COLS)\n",
    "X2_test_arr = df2_test_renamed[FEATURE_NAMES].values.astype(np.float32)\n",
    "y2_test_arr = df2_test_renamed['phishing'].values.astype(np.float32)\n",
    "\n",
    "# í†µí•© í…ŒìŠ¤íŠ¸\n",
    "X_test = np.vstack([X1_test_arr, X2_test_arr])\n",
    "y_test = np.hstack([y1_test_arr, y2_test_arr])\n",
    "\n",
    "# ì…”í”Œ\n",
    "indices = np.arange(len(X_val))\n",
    "np.random.shuffle(indices)\n",
    "X_val = X_val[indices]\n",
    "y_val = y_val[indices]\n",
    "\n",
    "indices = np.arange(len(X_test))\n",
    "np.random.shuffle(indices)\n",
    "X_test = X_test[indices]\n",
    "y_test = y_test[indices]\n",
    "\n",
    "print(f\"\\nâœ… Validation: {X_val.shape} - í”¼ì‹±:{int((y_val==1).sum())}, ì •ìƒ:{int((y_val==0).sum())}\")\n",
    "print(f\"âœ… Test (D1): {X1_test_arr.shape}\")\n",
    "print(f\"âœ… Test (D2): {X2_test_arr.shape}\")\n",
    "print(f\"âœ… Test (Merged): {X_test.shape} - í”¼ì‹±:{int((y_test==1).sum())}, ì •ìƒ:{int((y_test==0).sum())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 6) ë°ì´í„° ì •ê·œí™” ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ë°ì´í„° ì •ê·œí™” (RobustScaler)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ë¡œ fit\n",
    "X_merged_train_scaled = robust_scaler.fit_transform(X_merged_train).astype(np.float32)\n",
    "X_val_scaled = robust_scaler.transform(X_val).astype(np.float32)\n",
    "X_test_scaled = robust_scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "print(f\"âœ… RobustScaler ì ìš© ì™„ë£Œ\")\n",
    "print(f\"   - Train: min={X_merged_train_scaled.min():.4f}, max={X_merged_train_scaled.max():.4f}\")\n",
    "print(f\"   - Train: mean={X_merged_train_scaled.mean():.4f}, std={X_merged_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 7) Autoencoder ëª¨ë¸ êµ¬ì¶• ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ—ï¸ Autoencoder ëª¨ë¸ êµ¬ì¶•\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "input_dim = X_merged_train_scaled.shape[1]\n",
    "\n",
    "# Encoder\n",
    "encoder_input = keras.Input(shape=(input_dim,), name='encoder_input')\n",
    "\n",
    "x = layers.Dense(64, activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=keras.regularizers.l2(L2_REG),\n",
    "                 name='encoder_dense1')(encoder_input)\n",
    "\n",
    "x = layers.Dense(32, activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=keras.regularizers.l2(L2_REG),\n",
    "                 name='encoder_dense2')(x)\n",
    "\n",
    "# ì„ë² ë”© ë ˆì´ì–´\n",
    "embedding = layers.Dense(EMBEDDING_DIM, activation='relu',\n",
    "                          kernel_regularizer=keras.regularizers.l2(L2_REG),\n",
    "                          name='embedding')(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.Dense(32, activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=keras.regularizers.l2(L2_REG),\n",
    "                 name='decoder_dense1')(embedding)\n",
    "\n",
    "x = layers.Dense(64, activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=keras.regularizers.l2(L2_REG),\n",
    "                 name='decoder_dense2')(x)\n",
    "\n",
    "decoder_output = layers.Dense(input_dim, activation='linear',\n",
    "                               name='decoder_output')(x)\n",
    "\n",
    "# Autoencoder ëª¨ë¸\n",
    "autoencoder = Model(encoder_input, decoder_output, name='autoencoder')\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ (inferenceìš©)\n",
    "embedding_model = Model(encoder_input, embedding, name='embedding_model')\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "print(f\"\\nâœ… ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ\")\n",
    "print(f\"\\nAutoencoder êµ¬ì¡°:\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 8) ëª¨ë¸ í•™ìŠµ ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ì½œë°± ì„¤ì •\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# í•™ìŠµ\n",
    "history = autoencoder.fit(\n",
    "    X_merged_train_scaled, X_merged_train_scaled,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val_scaled, X_val_scaled),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ\")\n",
    "print(f\"   - ìµœì¢… Train Loss: {history.history['loss'][-1]:.6f}\")\n",
    "print(f\"   - ìµœì¢… Val Loss: {history.history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 9) ì„ë² ë”© ë²¡í„° ìƒì„± ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” ì„ë² ë”© ë²¡í„° ìƒì„±\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ëª¨ë“  ë°ì´í„°ì˜ ì„ë² ë”© ìƒì„±\n",
    "embeddings_train = embedding_model.predict(X_merged_train_scaled, verbose=0)\n",
    "embeddings_test = embedding_model.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "print(f\"\\nâœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"   - Train ì„ë² ë”©: {embeddings_train.shape}\")\n",
    "print(f\"   - Test ì„ë² ë”©: {embeddings_test.shape}\")\n",
    "\n",
    "# ì •ìƒ/í”¼ì‹± í”„ë¡œí† íƒ€ì… ê³„ì‚°\n",
    "legit_mask = y_merged_train == 0\n",
    "phish_mask = y_merged_train == 1\n",
    "\n",
    "prototype_legit = embeddings_train[legit_mask].mean(axis=0, keepdims=True)\n",
    "prototype_phish = embeddings_train[phish_mask].mean(axis=0, keepdims=True)\n",
    "\n",
    "print(f\"\\nâœ… í”„ë¡œí† íƒ€ì… ê³„ì‚° ì™„ë£Œ\")\n",
    "print(f\"   - ì •ìƒ í”„ë¡œí† íƒ€ì…: {prototype_legit.shape}\")\n",
    "print(f\"   - í”¼ì‹± í”„ë¡œí† íƒ€ì…: {prototype_phish.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 10) ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜ í‰ê°€ ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜ í‰ê°€\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "sim_legit = cosine_similarity(embeddings_test, prototype_legit).flatten()\n",
    "sim_phish = cosine_similarity(embeddings_test, prototype_phish).flatten()\n",
    "\n",
    "# ì˜ˆì¸¡: ì •ìƒì— ë” ê°€ê¹Œìš°ë©´ 0, í”¼ì‹±ì— ë” ê°€ê¹Œìš°ë©´ 1\n",
    "y_pred_emb = (sim_phish > sim_legit).astype(int)\n",
    "y_pred_proba_emb = sim_phish / (sim_legit + sim_phish)  # ì •ê·œí™”ëœ í™•ë¥ \n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€\n",
    "acc_emb = accuracy_score(y_test, y_pred_emb)\n",
    "prec_emb = precision_score(y_test, y_pred_emb, zero_division=0)\n",
    "rec_emb = recall_score(y_test, y_pred_emb, zero_division=0)\n",
    "f1_emb = f1_score(y_test, y_pred_emb, zero_division=0)\n",
    "auc_emb = roc_auc_score(y_test, y_pred_proba_emb)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜ ì„±ëŠ¥:\")\n",
    "print(f\"   - Accuracy:  {acc_emb:.4f}\")\n",
    "print(f\"   - Precision: {prec_emb:.4f}\")\n",
    "print(f\"   - Recall:    {rec_emb:.4f}\")\n",
    "print(f\"   - F1 Score:  {f1_emb:.4f}\")\n",
    "print(f\"   - AUC:       {auc_emb:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_test, y_pred_emb, target_names=['ì •ìƒ(0)', 'í”¼ì‹±(1)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 11) ì‹œê°í™” - í•™ìŠµ ê³¡ì„  ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê³¡ì„ \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "ax.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "ax.set_title('Autoencoder Learning Curve (19 Features)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Learning curve visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 12) ì‹œê°í™” - ROC ê³¡ì„  ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC ê³¡ì„ \n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_emb)\n",
    "\n",
    "ax.plot(fpr, tpr, 'b-', linewidth=2.5, label=f'Embedding Model (AUC={auc_emb:.4f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random Classifier')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curve - Embedding Model (19 Features)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… ROC curve visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 13) ì‹œê°í™” - Confusion Matrix ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_emb)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Legitimate', 'Phishing'],\n",
    "            yticklabels=['Legitimate', 'Phishing'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "\n",
    "ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Confusion Matrix - Embedding Model (19 Features)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Confusion matrix visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 14) ì‹œê°í™” - ì„ë² ë”© ê³µê°„ ë¶„í¬ (t-SNE) ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNEë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ê³µê°„ ì‹œê°í™”\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"t-SNE ì²˜ë¦¬ ì¤‘ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...\")\n",
    "\n",
    "# ìƒ˜í”Œë§ (ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ëŠë¦¬ë¯€ë¡œ 10,000ê°œë§Œ ì‚¬ìš©)\n",
    "sample_size = min(10000, len(embeddings_test))\n",
    "indices = np.random.choice(len(embeddings_test), sample_size, replace=False)\n",
    "\n",
    "embeddings_test_sample = embeddings_test[indices]\n",
    "y_test_sample = y_test[indices]\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=RANDOM_SEED, n_iter=1000)\n",
    "embeddings_2d = tsne.fit_transform(embeddings_test_sample)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "legit_mask = y_test_sample == 0\n",
    "phish_mask = y_test_sample == 1\n",
    "\n",
    "ax.scatter(embeddings_2d[legit_mask, 0], embeddings_2d[legit_mask, 1],\n",
    "           c='blue', label='Legitimate', alpha=0.6, s=30, edgecolors='none')\n",
    "ax.scatter(embeddings_2d[phish_mask, 0], embeddings_2d[phish_mask, 1],\n",
    "           c='red', label='Phishing', alpha=0.6, s=30, edgecolors='none')\n",
    "\n",
    "# í”„ë¡œí† íƒ€ì… í‘œì‹œ\n",
    "prototype_legit_2d = tsne.transform(prototype_legit)\n",
    "prototype_phish_2d = tsne.transform(prototype_phish)\n",
    "\n",
    "ax.scatter(prototype_legit_2d[:, 0], prototype_legit_2d[:, 1],\n",
    "           c='darkblue', marker='*', s=500, edgecolors='black', linewidths=2, label='Legit Prototype')\n",
    "ax.scatter(prototype_phish_2d[:, 0], prototype_phish_2d[:, 1],\n",
    "           c='darkred', marker='*', s=500, edgecolors='black', linewidths=2, label='Phish Prototype')\n",
    "\n",
    "ax.set_xlabel('t-SNE 1', fontsize=12)\n",
    "ax.set_ylabel('t-SNE 2', fontsize=12)\n",
    "ax.set_title('Embedding Space Distribution (t-SNE) - 19 Features', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… t-SNE visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 15) ê²°ê³¼ ì €ì¥ ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_dir = '/home/wza/QR_Phishing/phishing/Deviation-in-Feature-Contribution-7760/Code'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
    "autoencoder.save(f'{output_dir}/embedding_model_19features.h5')\n",
    "print(f\"âœ… Autoencoder ëª¨ë¸ ì €ì¥: {output_dir}/embedding_model_19features.h5\")\n",
    "\n",
    "# 2. í”„ë¡œí† íƒ€ì… ë²¡í„° ì €ì¥\n",
    "np.save(f'{output_dir}/prototype_legit_19features.npy', prototype_legit)\n",
    "np.save(f'{output_dir}/prototype_phish_19features.npy', prototype_phish)\n",
    "print(f\"âœ… í”„ë¡œí† íƒ€ì… ë²¡í„° ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# 3. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'],\n",
    "    'Embedding_Model': [acc_emb, prec_emb, rec_emb, f1_emb, auc_emb]\n",
    "})\n",
    "results_df.to_csv(f'{output_dir}/embedding_model_19features_performance.csv', index=False)\n",
    "print(f\"âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥: {output_dir}/embedding_model_19features_performance.csv\")\n",
    "\n",
    "# 4. Scaler íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "scaler_params = {\n",
    "    'type': 'robust_scaler',\n",
    "    'center': robust_scaler.center_.tolist(),\n",
    "    'scale': robust_scaler.scale_.tolist(),\n",
    "    'feature_names': FEATURE_NAMES\n",
    "}\n",
    "with open(f'{output_dir}/scaler_params_19features.json', 'w') as f:\n",
    "    json.dump(scaler_params, f, indent=2)\n",
    "print(f\"âœ… Scaler íŒŒë¼ë¯¸í„° ì €ì¥: {output_dir}/scaler_params_19features.json\")\n",
    "\n",
    "print(f\"\\nğŸ“ ëª¨ë“  íŒŒì¼ ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===== 16) ìµœì¢… ìš”ì•½ ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‹ ìµœì¢… ìš”ì•½\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nâœ… ë°ì´í„° ì •ë³´:\")\n",
    "print(f\"   - í”¼ì²˜ ê°œìˆ˜: {len(FEATURE_NAMES)}\")\n",
    "print(f\"   - í•™ìŠµ ë°ì´í„°: {X_merged_train_scaled.shape[0]:,} ìƒ˜í”Œ\")\n",
    "print(f\"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test_scaled.shape[0]:,} ìƒ˜í”Œ\")\n",
    "\n",
    "print(f\"\\nâœ… ëª¨ë¸ ì„¤ì •:\")\n",
    "print(f\"   - í”„ë ˆì„ì›Œí¬: TensorFlow Keras (Autoencoder)\")\n",
    "print(f\"   - ì„ë² ë”© ì°¨ì›: {EMBEDDING_DIM}\")\n",
    "print(f\"   - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
    "print(f\"   - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   - L2 Regularization: {L2_REG}\")\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… ì„±ëŠ¥ (ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜):\")\n",
    "print(f\"   - Accuracy:  {acc_emb:.4f}\")\n",
    "print(f\"   - Precision: {prec_emb:.4f}\")\n",
    "print(f\"   - Recall:    {rec_emb:.4f}\")\n",
    "print(f\"   - F1 Score:  {f1_emb:.4f}\")\n",
    "print(f\"   - AUC:       {auc_emb:.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… ì €ì¥ëœ íŒŒì¼:\")\n",
    "print(f\"   - embedding_model_19features.h5\")\n",
    "print(f\"   - prototype_legit_19features.npy\")\n",
    "print(f\"   - prototype_phish_19features.npy\")\n",
    "print(f\"   - embedding_model_19features_performance.csv\")\n",
    "print(f\"   - scaler_params_19features.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
