{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ëª¨ë¸ ì´ˆê¸°í™” (Linux NVIDIA GPU) =====\n",
    "import os\n",
    "\n",
    "# â­ pip CUDAë¥¼ ì‹œìŠ¤í…œ CUDAë³´ë‹¤ ë¨¼ì € ì‚¬ìš©í•˜ë„ë¡ PATH ì„¤ì •\n",
    "# ì‹œìŠ¤í…œ ptxas(12.5)ì™€ pip CUDA(12.9) ë²„ì „ ì¶©ëŒ í•´ê²°\n",
    "CUDA_NVCC_PATH = \"/home/yu_mcc/miniconda3/envs/tf_gpu/lib/python3.10/site-packages/nvidia/cuda_nvcc/bin\"\n",
    "os.environ['PATH'] = CUDA_NVCC_PATH + \":\" + os.environ.get('PATH', '')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "\n",
    "# NVIDIA GPU í™•ì¸ ë° í™œì„±í™”\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"âœ… NVIDIA GPU ë°œê²¬: {len(gpus)}ê°œ\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"   - {gpu.name}\")\n",
    "    # GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ (í•„ìš”í•œ ë§Œí¼ë§Œ ì‚¬ìš©)\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"âœ… GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ í™œì„±í™”ë¨!\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ ë©”ëª¨ë¦¬ ì„¤ì • ì‹¤íŒ¨: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ GPUê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# GPU ì—°ì‚° í…ŒìŠ¤íŠ¸ (ì—ëŸ¬ ë°œìƒì‹œ CPUë¡œ í´ë°±)\n",
    "print(\"\\n=== GPU ì—°ì‚° í…ŒìŠ¤íŠ¸ ===\")\n",
    "try:\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.random.normal([1000, 1000])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(f\"âœ… GPU ì—°ì‚° í…ŒìŠ¤íŠ¸ ì™„ë£Œ: shape = {c.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ GPU ì—°ì‚° ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}\")\n",
    "\n",
    "# Keras ë°±ì—”ë“œ ì´ˆê¸°í™”\n",
    "tf.keras.backend.clear_session()\n",
    "print(\"\\nâœ… TensorFlow ë° GPU ì´ˆê¸°í™” ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) í™˜ê²½ ì„¤ì • ë° Import =====\n",
    "import os\n",
    "os.environ['CONDA_DEFAULT_ENV'] = 'tf_gpu'\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "print(\"TensorFlow ë²„ì „:\", tf.__version__)\n",
    "print(\"NumPy ë²„ì „:\", np.__version__)\n",
    "print(\"Pandas ë²„ì „:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1) ì„¤ì • =====\n",
    "# ğŸ”§ Linux í™˜ê²½ ê²½ë¡œ\n",
    "DATA_PATH = \"/home/yu_mcc/QR_Phishing/phishing/phishing_data_tflite_ready.csv\"\n",
    "TARGET_COL = \"status\"\n",
    "EMBEDDING_DIM = 32  # ì„ë² ë”© ì°¨ì› (ì›ë˜ ì„¤ì •ìœ¼ë¡œ ë³µì›)\n",
    "BATCH_SIZE = 16  # ë°°ì¹˜ í¬ê¸° (ì›ë˜ ì„¤ì •ìœ¼ë¡œ ë³µì›)\n",
    "EPOCHS = 200  # ì—í­ (ì›ë˜ ì„¤ì •ìœ¼ë¡œ ë³µì›)\n",
    "LEARNING_RATE = 0.0001  # ë¡œìŠ¤ í­ë°œ ë°©ì§€: 0.001 â†’ 0.0001\n",
    "RANDOM_SEED = 75\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"ì„¤ì • ì™„ë£Œ: ì„ë² ë”© ì°¨ì›={EMBEDDING_DIM}, ë°°ì¹˜ í¬ê¸°={BATCH_SIZE}, ì—í­={EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2) ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ =====\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"ë°ì´í„° shape: {df.shape}\")\n",
    "print(f\"\\nì»¬ëŸ¼ ëª©ë¡:\\n{df.columns.tolist()}\")\n",
    "print(f\"\\nì²˜ìŒ 5í–‰:\\n{df.head()}\")\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸\n",
    "print(f\"\\níƒ€ê²Ÿ ë³€ìˆ˜ ({TARGET_COL}) ë¶„í¬:\")\n",
    "print(df[TARGET_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3) í”¼ì²˜ ë° ë ˆì´ë¸” ë¶„ë¦¬ (50:50 ê· í˜• ë¶„í• ) =====\n",
    "# ì›ë³¸ ë°ì´í„° ë³´ì¡´ì„ ìœ„í•´ ë³µì‚¬ë³¸ ì‚¬ìš©\n",
    "df_work = df.copy()\n",
    "\n",
    "# 'id' ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì œê±°\n",
    "if 'id' in df_work.columns:\n",
    "    df_work = df_work.drop(columns=['id'])\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "# status: 0 = ì •ìƒ, 1 = í”¼ì‹±\n",
    "# ëª¨ë¸ ì¶œë ¥ë„ ë™ì¼: 0ì— ê°€ê¹Œìš°ë©´ ì •ìƒ, 1ì— ê°€ê¹Œìš°ë©´ í”¼ì‹±\n",
    "\n",
    "print(f\"í”¼ì²˜ ê°œìˆ˜: {len(df_work.columns) - 1}\")\n",
    "print(f\"ì „ì²´ ë°ì´í„° shape: {df_work.shape}\")\n",
    "print(f\"\\nì›ë³¸ ë ˆì´ë¸” ë¶„í¬:\")\n",
    "print(f\"  - ì •ìƒ(0): {(df_work[TARGET_COL]==0).sum()}\")\n",
    "print(f\"  - í”¼ì‹±(1): {(df_work[TARGET_COL]==1).sum()}\")\n",
    "\n",
    "# ===== 50:50 ê· í˜• ë¶„í•  í•¨ìˆ˜ =====\n",
    "def split_class(class_df, train_ratio=0.6, val_ratio=0.2, seed=42):\n",
    "    \"\"\"í´ë˜ìŠ¤ë³„ë¡œ train/val/test ë¶„í• \"\"\"\n",
    "    # ì—¬ëŸ¬ ë²ˆ ì…”í”Œí•´ì„œ íŒ¨í„´ í¸í–¥ ë°©ì§€\n",
    "    shuffled = class_df.sample(frac=1, random_state=seed)\n",
    "    shuffled = shuffled.sample(frac=1, random_state=seed+1)\n",
    "    shuffled = shuffled.sample(frac=1, random_state=seed+2)\n",
    "    \n",
    "    n = len(shuffled)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    return (shuffled.iloc[:train_end], \n",
    "            shuffled.iloc[train_end:val_end], \n",
    "            shuffled.iloc[val_end:])\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ë¡œ ë¶„ë¦¬\n",
    "df_legit = df_work[df_work[TARGET_COL] == 0]  # ì •ìƒ\n",
    "df_phish = df_work[df_work[TARGET_COL] == 1]  # í”¼ì‹±\n",
    "\n",
    "# ê° í´ë˜ìŠ¤ë³„ë¡œ 60/20/20 ë¶„í• \n",
    "legit_train, legit_val, legit_test = split_class(df_legit, seed=RANDOM_SEED)\n",
    "phish_train, phish_val, phish_test = split_class(df_phish, seed=RANDOM_SEED)\n",
    "\n",
    "# í•©ì¹˜ê¸° (ê° ì„¸íŠ¸ì—ì„œ 50:50 ë¹„ìœ¨ ìœ ì§€)\n",
    "train_df = pd.concat([legit_train, phish_train]).sample(frac=1, random_state=RANDOM_SEED)\n",
    "val_df = pd.concat([legit_val, phish_val]).sample(frac=1, random_state=RANDOM_SEED)\n",
    "test_df = pd.concat([legit_test, phish_test]).sample(frac=1, random_state=RANDOM_SEED)\n",
    "\n",
    "# í”¼ì²˜ì™€ ë ˆì´ë¸” ë¶„ë¦¬\n",
    "X_train = train_df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
    "y_train = train_df[TARGET_COL].values.astype(np.float32)\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
    "y_val = val_df[TARGET_COL].values.astype(np.float32)\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
    "y_test = test_df[TARGET_COL].values.astype(np.float32)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"\\n===== 50:50 ê· í˜• ë¶„í•  ê²°ê³¼ =====\")\n",
    "print(f\"Train: {X_train.shape} - ì •ìƒ:{(y_train==0).sum()}, í”¼ì‹±:{(y_train==1).sum()}\")\n",
    "print(f\"Val:   {X_val.shape} - ì •ìƒ:{(y_val==0).sum()}, í”¼ì‹±:{(y_val==1).sum()}\")\n",
    "print(f\"Test:  {X_test.shape} - ì •ìƒ:{(y_test==0).sum()}, í”¼ì‹±:{(y_test==1).sum()}\")\n",
    "\n",
    "# ë¹„ìœ¨ í™•ì¸\n",
    "print(f\"\\n===== ê° ì„¸íŠ¸ì˜ í”¼ì‹± ë¹„ìœ¨ =====\")\n",
    "print(f\"Train í”¼ì‹± ë¹„ìœ¨: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"Val í”¼ì‹± ë¹„ìœ¨:   {y_val.mean()*100:.1f}%\")\n",
    "print(f\"Test í”¼ì‹± ë¹„ìœ¨:  {y_test.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4) ë°ì´í„° ì „ì²˜ë¦¬ (RobustScalerë§Œ ì ìš©) =====\n",
    "# âœ… ì´ìƒì¹˜ì— ë¯¼ê°í•œ í”¼ì²˜: RobustScaler (ì¤‘ì•™ê°’/IQR ê¸°ë°˜) - ì´ìƒì¹˜ì— ë” ê°•í•¨\n",
    "# âœ… ë‚˜ë¨¸ì§€ í”¼ì²˜: ì •ê·œí™” ì—†ìŒ (ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš© - ëª¨ë¸ì´ ì›ë˜ ìŠ¤ì¼€ì¼ ì§ì ‘ í•™ìŠµ)\n",
    "# Androidì—ì„œëŠ” RobustScaler íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©í•´ ì „ì²˜ë¦¬ í•„ìš”\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# RobustScalerë¥¼ ì ìš©í•  í”¼ì²˜ ëª©ë¡ (ì´ìƒì¹˜ê°€ ë§ì€ í”¼ì²˜ë“¤)\n",
    "# âš ï¸ ì œê±°ëœ í”¼ì²˜ (CSVì— ì—†ìŒ): nb_redirection, ratio_extRedirection, ratio_extErrors\n",
    "robust_cols = [\n",
    "    'length_url',\n",
    "    'length_hostname',\n",
    "    'nb_dots',\n",
    "    'nb_hyphens',\n",
    "    'nb_and',\n",
    "    'nb_eq',\n",
    "    'nb_underscore',\n",
    "    'nb_percent',\n",
    "    'nb_slash',\n",
    "    'nb_colon',\n",
    "    'nb_semicolumn',\n",
    "    'nb_space',\n",
    "    'nb_com',\n",
    "    'ratio_digits_url',\n",
    "    'ratio_digits_host',\n",
    "    'length_words_raw',\n",
    "    'char_repeat',\n",
    "    'shortest_words_raw',\n",
    "    'shortest_word_host',\n",
    "    'shortest_word_path',\n",
    "    'longest_words_raw',\n",
    "    'longest_word_host',\n",
    "    'longest_word_path',\n",
    "    'avg_words_raw',\n",
    "    'avg_word_host',\n",
    "    'avg_word_path',\n",
    "    'phish_hints',\n",
    "    'nb_extCSS',\n",
    "]\n",
    "\n",
    "# í”¼ì²˜ ì¸ë±ìŠ¤ ë§¤í•‘\n",
    "feature_names = [col for col in df_work.columns if col != TARGET_COL]\n",
    "robust_indices = [feature_names.index(col) for col in robust_cols if col in feature_names]\n",
    "raw_indices = [i for i in range(len(feature_names)) if i not in robust_indices]\n",
    "\n",
    "print(f\"RobustScaler ì ìš© í”¼ì²˜: {len(robust_indices)}ê°œ\")\n",
    "print(f\"ì •ê·œí™” ì—†ìŒ (ì›ë³¸ ê·¸ëŒ€ë¡œ): {len(raw_indices)}ê°œ\")\n",
    "\n",
    "# RobustScaler ìƒì„±\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰\n",
    "X_train_scaled = X_train.copy().astype(np.float32)\n",
    "X_val_scaled = X_val.copy().astype(np.float32)\n",
    "X_test_scaled = X_test.copy().astype(np.float32)\n",
    "\n",
    "# RobustScaler ì ìš©ë§Œ (ì¤‘ì•™ê°’ ê¸°ë°˜ - ì´ìƒì¹˜ ì˜í–¥ ì ìŒ)\n",
    "X_train_scaled[:, robust_indices] = robust_scaler.fit_transform(X_train[:, robust_indices]).astype(np.float32)\n",
    "X_val_scaled[:, robust_indices] = robust_scaler.transform(X_val[:, robust_indices]).astype(np.float32)\n",
    "X_test_scaled[:, robust_indices] = robust_scaler.transform(X_test[:, robust_indices]).astype(np.float32)\n",
    "\n",
    "# raw_indicesëŠ” ê·¸ëŒ€ë¡œ ë‘ê¸° (ì •ê·œí™” ì•ˆ í•¨)\n",
    "\n",
    "print(\"\\në°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ (RobustScalerë§Œ ì ìš©)\")\n",
    "print(f\"Train í”¼ì²˜ ë²”ìœ„: min={X_train_scaled.min():.2f}, max={X_train_scaled.max():.2f}\")\n",
    "print(f\"Train í”¼ì²˜ í‰ê· : {X_train_scaled.mean():.4f}, í‘œì¤€í¸ì°¨: {X_train_scaled.std():.4f}\")\n",
    "print(f\"í”¼ì²˜ ê°œìˆ˜: {X_train_scaled.shape[1]}ê°œ\")\n",
    "\n",
    "# Androidì—ì„œ ì‚¬ìš©í•  íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "scaler_params = {\n",
    "    'type': 'robust_only',\n",
    "    'robust_cols': robust_cols,\n",
    "    'robust_center': robust_scaler.center_.tolist(),\n",
    "    'robust_scale': robust_scaler.scale_.tolist(),\n",
    "    'raw_cols': [feature_names[i] for i in raw_indices]\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“Š Scaler íŒŒë¼ë¯¸í„° (Android ì „ì²˜ë¦¬ìš©):\")\n",
    "print(f\"   RobustScaler: {len(robust_cols)}ê°œ í”¼ì²˜\")\n",
    "print(f\"   ì›ë³¸ ê·¸ëŒ€ë¡œ: {len(raw_indices)}ê°œ í”¼ì²˜)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 5) ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶• (TFLite ì¹œí™”ì : ì •ê·œí™” ë ˆì´ì–´ ì—†ìŒ) =====\n",
    "# âœ… Dense + ReLU + L2 regularizationë§Œ ì‚¬ìš©\n",
    "# âœ… ì…ë ¥ì€ RobustScalerë§Œ ì ìš© (40ê°œ í”¼ì²˜ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ)\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "L2_REG = 0.0004  # ë¡œìŠ¤ í­ë°œ ë°©ì§€: 0.001 â†’ 0.0001\n",
    "\n",
    "encoder_input = keras.Input(shape=(input_dim,), name='encoder_input')\n",
    "\n",
    "# Encoder ë¸”ë¡ (ì •ê·œí™” ë ˆì´ì–´ ì—†ì´ L2 regularizationë§Œ ì‚¬ìš© - Dropout ì œê±°)\n",
    "# â­ TFLite í˜¸í™˜ì„±: Dropout ì œê±° (TFLiteì—ì„œ Dropoutì´ í™œì„±í™” ìƒíƒœë¡œ ë‚¨ì„ ìˆ˜ ìˆìŒ)\n",
    "x = layers.Dense(256, activation='relu', \n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=keras.regularizers.l2(L2_REG),\n",
    "                 name='encoder_dense1')(encoder_input)\n",
    "\n",
    "x = layers.Dense(128, activation='relu',\n",
    "                 kernel_initializer='he_normal', \n",
    "                 kernel_regularizer=keras.regularizers.l2(L2_REG),\n",
    "                 name='encoder_dense2')(x)\n",
    "\n",
    "x = layers.Dense(64, activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=keras.regularizers.l2(L2_REG),\n",
    "                 name='encoder_dense3')(x)\n",
    "\n",
    "# ì„ë² ë”© ë ˆì´ì–´\n",
    "embedding = layers.Dense(EMBEDDING_DIM, activation='relu', \n",
    "                         kernel_regularizer=keras.regularizers.l2(L2_REG),\n",
    "                         name='embedding')(x)\n",
    "\n",
    "# Classification Head (Dropout ì œê±°)\n",
    "x = layers.Dense(32, activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=keras.regularizers.l2(L2_REG),\n",
    "                 name='classifier_dense1')(embedding)\n",
    "\n",
    "x = layers.Dense(16, activation='relu',\n",
    "                 kernel_regularizer=keras.regularizers.l2(L2_REG),\n",
    "                 name='classifier_dense2')(x)\n",
    "output = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "# ì „ì²´ ë¶„ë¥˜ ëª¨ë¸\n",
    "classifier = Model(inputs=encoder_input, outputs=output, name='phishing_classifier')\n",
    "\n",
    "# Encoderë§Œ ë”°ë¡œ (ì„ë² ë”© ì¶”ì¶œìš©)\n",
    "encoder = Model(inputs=encoder_input, outputs=embedding, name='encoder')\n",
    "\n",
    "classifier.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"\\n===== ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¡° (TFLite ì¹œí™”ì : ì •ê·œí™” ë ˆì´ì–´ ì—†ìŒ) =====\")\n",
    "print(\"âœ… Dense + ReLU + L2 regularizationë§Œ ì‚¬ìš©\")\n",
    "print(\"âœ… ì…ë ¥: RobustScaler 31ê°œ í”¼ì²˜ + ì›ë³¸ 40ê°œ í”¼ì²˜\")\n",
    "classifier.summary()\n",
    "print(\"\\n===== Encoder ëª¨ë¸ êµ¬ì¡° =====\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 6) ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ =====\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = classifier.fit(\n",
    "    X_train_scaled, y_train,  # í”¼ì²˜ -> ë ˆì´ë¸” (ë¶„ë¥˜ í•™ìŠµ)\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\në¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 7) í•™ìŠµ ê³¡ì„  ì‹œê°í™” =====\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (Binary Crossentropy)')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# AUC\n",
    "axes[2].plot(history.history['auc'], label='Train AUC')\n",
    "axes[2].plot(history.history['val_auc'], label='Val AUC')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('AUC')\n",
    "axes[2].set_title('Training and Validation AUC')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 8) ëª¨ë¸ í‰ê°€ (Validation) =====\n",
    "print(\"=\"*60)\n",
    "print(\"ëª¨ë¸ í‰ê°€ (Validation Set)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "pred_val = classifier.predict(X_val_scaled, verbose=0)\n",
    "pred_val_proba = pred_val.flatten()\n",
    "pred_val_binary = (pred_val_proba > 0.5).astype(int)\n",
    "\n",
    "# ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "val_loss, val_acc, val_auc = classifier.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "val_precision = precision_score(y_val, pred_val_binary)\n",
    "val_recall = recall_score(y_val, pred_val_binary)\n",
    "val_f1 = f1_score(y_val, pred_val_binary)\n",
    "\n",
    "print(f\"\\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:\")\n",
    "print(f\"  - Accuracy:  {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "print(f\"  - Precision: {val_precision:.4f}\")\n",
    "print(f\"  - Recall:    {val_recall:.4f}\")\n",
    "print(f\"  - F1 Score:  {val_f1:.4f}\")\n",
    "print(f\"  - AUC-ROC:   {val_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_val, pred_val_binary, target_names=['Legitimate', 'Phishing']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 9) ëª¨ë¸ í‰ê°€ (Test) =====\n",
    "print(\"=\"*60)\n",
    "print(\"ëª¨ë¸ í‰ê°€ (Test Set)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "pred_test = classifier.predict(X_test_scaled, verbose=0)\n",
    "pred_test_proba = pred_test.flatten()\n",
    "pred_test_binary = (pred_test_proba > 0.5).astype(int)\n",
    "\n",
    "# ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "test_loss, test_acc, test_auc = classifier.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "test_precision = precision_score(y_test, pred_test_binary)\n",
    "test_recall = recall_score(y_test, pred_test_binary)\n",
    "test_f1 = f1_score(y_test, pred_test_binary)\n",
    "\n",
    "print(f\"\\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:\")\n",
    "print(f\"  - Accuracy:  {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"  - Precision: {test_precision:.4f}\")\n",
    "print(f\"  - Recall:    {test_recall:.4f}\")\n",
    "print(f\"  - F1 Score:  {test_f1:.4f}\")\n",
    "print(f\"  - AUC-ROC:   {test_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_test, pred_test_binary, target_names=['Legitimate', 'Phishing']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 10) ROC ì»¤ë¸Œ ë° Confusion Matrix ì‹œê°í™” =====\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred_test)\n",
    "axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {test_auc:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, pred_test_binary)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Legitimate', 'Phishing'],\n",
    "            yticklabels=['Legitimate', 'Phishing'])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 10-1) ìµœì  ì„ê³„ê°’ ë¶„ì„ =====\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ ìµœì  ì„ê³„ê°’ ë¶„ì„\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œ F1 Score ê³„ì‚° (0.05 ~ 0.95)\n",
    "thresholds_to_test = np.arange(0.05, 1.0, 0.05)\n",
    "f1_scores = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    pred_binary = (pred_test_proba > thresh).astype(int)\n",
    "    f1 = f1_score(y_test, pred_binary)\n",
    "    acc = accuracy_score(y_test, pred_binary)\n",
    "    prec = precision_score(y_test, pred_binary, zero_division=0)\n",
    "    rec = recall_score(y_test, pred_binary, zero_division=0)\n",
    "    f1_scores.append(f1)\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "# ìµœì  ì„ê³„ê°’ ì°¾ê¸° (F1 ê¸°ì¤€)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds_to_test[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "# ê¸°ë³¸ 0.5 ì„±ëŠ¥\n",
    "default_f1 = f1_score(y_test, (pred_test_proba > 0.5).astype(int))\n",
    "default_acc = accuracy_score(y_test, (pred_test_proba > 0.5).astype(int))\n",
    "\n",
    "print(f\"\\nğŸ“Š ì„ê³„ê°’ë³„ F1 Score (ìƒìœ„ 10ê°œ):\")\n",
    "sorted_indices = np.argsort(f1_scores)[::-1][:10]\n",
    "for idx in sorted_indices:\n",
    "    thresh = thresholds_to_test[idx]\n",
    "    f1 = f1_scores[idx]\n",
    "    marker = \" â­ BEST\" if idx == best_idx else \"\"\n",
    "    default_marker = \" (ê¸°ë³¸ê°’)\" if abs(thresh - 0.5) < 0.01 else \"\"\n",
    "    print(f\"   Threshold {thresh:.2f}: F1 = {f1:.4f}{marker}{default_marker}\")\n",
    "\n",
    "print(f\"\\nâœ… ìµœì  ì„ê³„ê°’: {best_threshold:.2f}\")\n",
    "print(f\"   - F1 Score: {best_f1:.4f}\")\n",
    "print(f\"   - Accuracy: {accuracies[best_idx]:.4f}\")\n",
    "print(f\"   - Precision: {precisions[best_idx]:.4f}\")\n",
    "print(f\"   - Recall: {recalls[best_idx]:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Œ ê¸°ë³¸ ì„ê³„ê°’ 0.5 ì„±ëŠ¥:\")\n",
    "print(f\"   - F1 Score: {default_f1:.4f}\")\n",
    "print(f\"   - Accuracy: {default_acc:.4f}\")\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 vs Threshold\n",
    "axes[0].plot(thresholds_to_test, f1_scores, 'b-', linewidth=2, label='F1 Score')\n",
    "axes[0].plot(thresholds_to_test, accuracies, 'g--', linewidth=2, label='Accuracy')\n",
    "axes[0].axvline(x=best_threshold, color='red', linestyle='--', linewidth=2, label=f'Best: {best_threshold:.2f}')\n",
    "axes[0].axvline(x=0.5, color='orange', linestyle=':', linewidth=2, label='Default: 0.5')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('F1 Score & Accuracy vs Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ (í´ë˜ìŠ¤ë³„) + ì„ê³„ê°’ í‘œì‹œ\n",
    "axes[1].hist(pred_test_proba[y_test==0], bins=30, alpha=0.6, label='Legitimate (0)', color='blue')\n",
    "axes[1].hist(pred_test_proba[y_test==1], bins=30, alpha=0.6, label='Phishing (1)', color='red')\n",
    "axes[1].axvline(x=best_threshold, color='red', linestyle='--', linewidth=2, label=f'Best: {best_threshold:.2f}')\n",
    "axes[1].axvline(x=0.5, color='orange', linestyle=':', linewidth=2, label='Default: 0.5')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Prediction Distribution by Class')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ê²°ë¡ \n",
    "diff = abs(best_threshold - 0.5)\n",
    "f1_improvement = best_f1 - default_f1\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ ê²°ë¡ \")\n",
    "print(\"=\"*60)\n",
    "if diff < 0.1 and f1_improvement < 0.01:\n",
    "    print(f\"âœ… 0.5ì™€ ìµœì  ì„ê³„ê°’ ì°¨ì´ê°€ {diff:.2f}ë¡œ ì‘ê³ , F1 ê°œì„ ë„ {f1_improvement:.4f}ë¡œ ë¯¸ë¯¸í•©ë‹ˆë‹¤.\")\n",
    "    print(f\"   â†’ ê¸°ë³¸ê°’ 0.5 ì‚¬ìš©í•´ë„ ë©ë‹ˆë‹¤.\")\n",
    "    FINAL_THRESHOLD = 0.5\n",
    "else:\n",
    "    print(f\"âš ï¸ ìµœì  ì„ê³„ê°’({best_threshold:.2f})ì´ 0.5ì™€ {diff:.2f} ì°¨ì´ë‚©ë‹ˆë‹¤.\")\n",
    "    print(f\"   F1 ê°œì„ : {default_f1:.4f} â†’ {best_f1:.4f} (+{f1_improvement:.4f})\")\n",
    "    print(f\"   â†’ Android ì•±ì—ì„œ {best_threshold:.2f}ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\")\n",
    "    FINAL_THRESHOLD = best_threshold\n",
    "\n",
    "print(f\"\\nğŸ¯ ìµœì¢… ì‚¬ìš© ì„ê³„ê°’: {FINAL_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 11) ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ ì‹œê°í™” =====\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "legit_mask = (y_test == 0)  # ì •ìƒ\n",
    "phish_mask = (y_test == 1)  # í”¼ì‹±\n",
    "\n",
    "# ì •ìƒ ì‚¬ì´íŠ¸ì˜ ì˜ˆì¸¡ í™•ë¥  ë¶„í¬\n",
    "axes[0].hist(pred_test[legit_mask], bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
    "axes[0].set_xlabel('Predicted Probability (0=Legitimate)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Prediction Distribution for Legitimate Samples')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# í”¼ì‹± ì‚¬ì´íŠ¸ì˜ ì˜ˆì¸¡ í™•ë¥  ë¶„í¬\n",
    "axes[1].hist(pred_test[phish_mask], bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[1].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
    "axes[1].set_xlabel('Predicted Probability (1=Phishing)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Prediction Distribution for Phishing Samples')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nì •ìƒ ì‚¬ì´íŠ¸ í‰ê·  ì˜ˆì¸¡ê°’: {pred_test[legit_mask].mean():.4f}\")\n",
    "print(f\"í”¼ì‹± ì‚¬ì´íŠ¸ í‰ê·  ì˜ˆì¸¡ê°’: {pred_test[phish_mask].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 12) ì„ë² ë”© ê³µê°„ ì‹œê°í™” (t-SNE) =====\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Encoderë¡œ ì„ë² ë”© ì¶”ì¶œ\n",
    "embeddings_test = encoder.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "# t-SNE ë³€í™˜ (2D)\n",
    "print(\"t-SNE ë³€í™˜ ì¤‘...\")\n",
    "tsne = TSNE(n_components=2, random_state=RANDOM_SEED, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(embeddings_test)\n",
    "\n",
    "# ì‹œê°í™” (0=íŒŒë‘ ì •ìƒ, 1=ë¹¨ê°• í”¼ì‹±)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# ì •ìƒ (0) - íŒŒë‘\n",
    "legit_mask = y_test == 0\n",
    "plt.scatter(embeddings_2d[legit_mask, 0], embeddings_2d[legit_mask, 1], \n",
    "            c='blue', alpha=0.6, s=10, label='Legitimate (0)')\n",
    "\n",
    "# í”¼ì‹± (1) - ë¹¨ê°•\n",
    "phishing_mask = y_test == 1\n",
    "plt.scatter(embeddings_2d[phishing_mask, 0], embeddings_2d[phishing_mask, 1], \n",
    "            c='red', alpha=0.6, s=10, label='Phishing (1)')\n",
    "\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('Embedding Space Visualization (t-SNE)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 12-1) SHAP í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ (ê³„ì‚°) =====\n",
    "# ì–´ë–¤ í”¼ì²˜ê°€ ëª¨ë¸ ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë¶„ì„\n",
    "\n",
    "# SHAP ì„¤ì¹˜ í™•ì¸\n",
    "try:\n",
    "    import shap\n",
    "    print(f\"âœ… SHAP ë²„ì „: {shap.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ SHAPê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜ ì¤‘...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'shap', '-q'])\n",
    "    import shap\n",
    "    print(f\"âœ… SHAP ì„¤ì¹˜ ì™„ë£Œ: {shap.__version__}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ” SHAP í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ (54ê°œ ì „ì²´ í”¼ì²˜)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ë°°ê²½ ë°ì´í„° ìƒ˜í”Œë§ (ê³„ì‚° ì†ë„ë¥¼ ìœ„í•´ 100ê°œë§Œ ì‚¬ìš©)\n",
    "background_samples = 100\n",
    "np.random.seed(RANDOM_SEED)\n",
    "background_idx = np.random.choice(len(X_train_scaled), background_samples, replace=False)\n",
    "background = X_train_scaled[background_idx]\n",
    "\n",
    "print(f\"\\n1ï¸âƒ£ ë°°ê²½ ë°ì´í„°: {background.shape[0]}ê°œ ìƒ˜í”Œ\")\n",
    "\n",
    "# SHAP Explainer ìƒì„±\n",
    "print(\"2ï¸âƒ£ SHAP Explainer ìƒì„± ì¤‘...\")\n",
    "try:\n",
    "    # DeepExplainer ì‹œë„ (TensorFlow/Keras ëª¨ë¸ìš©, ë” ë¹ ë¦„)\n",
    "    explainer = shap.DeepExplainer(classifier, background)\n",
    "    print(\"   âœ… DeepExplainer ì‚¬ìš© (ë¹ ë¥¸ ê³„ì‚°)\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ DeepExplainer ì‹¤íŒ¨: {e}\")\n",
    "    print(\"   â†’ KernelExplainerë¡œ ì „í™˜ (ëŠë¦¬ì§€ë§Œ ë²”ìš©)\")\n",
    "    explainer = shap.KernelExplainer(\n",
    "        lambda x: classifier.predict(x, verbose=0), \n",
    "        background\n",
    "    )\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì— ëŒ€í•œ SHAP ê°’ ê³„ì‚°\n",
    "test_samples_for_shap = 200  # ë¶„ì„í•  í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜\n",
    "test_idx = np.random.choice(len(X_test_scaled), test_samples_for_shap, replace=False)\n",
    "X_test_sample = X_test_scaled[test_idx]\n",
    "\n",
    "print(f\"3ï¸âƒ£ SHAP ê°’ ê³„ì‚° ì¤‘ ({test_samples_for_shap}ê°œ ìƒ˜í”Œ)...\")\n",
    "print(\"   â³ ì´ ì‘ì—…ì€ 2-5ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...\")\n",
    "\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "# SHAP ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬ (ì´ì§„ ë¶„ë¥˜)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[0]\n",
    "\n",
    "print(f\"âœ… SHAP ê°’ ê³„ì‚° ì™„ë£Œ! Shape: {shap_values.shape}\")\n",
    "\n",
    "# í”¼ì²˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°\n",
    "feature_names = [col for col in df_work.columns if col != TARGET_COL]\n",
    "print(f\"âœ… ë¶„ì„ ëŒ€ìƒ í”¼ì²˜: {len(feature_names)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 12-2) SHAP Horizontal Bar Chart (ë…¼ë¬¸ìš© íˆ¬ì»¬ëŸ¼) =====\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"/home/yu_mcc/QR_Phishing/phishing\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SHAP Horizontal Bar Chart (54 Features - ë…¼ë¬¸ìš© íˆ¬ì»¬ëŸ¼)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# SHAP ê°’ ê³„ì‚°\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"Feature names count: {len(feature_names)}\")\n",
    "\n",
    "mean_shap_signed = np.mean(shap_values, axis=0)\n",
    "mean_abs_shap = np.abs(mean_shap_signed)\n",
    "\n",
    "if isinstance(mean_shap_signed, np.ndarray) and len(mean_shap_signed.shape) > 1:\n",
    "    mean_shap_signed = mean_shap_signed.flatten()\n",
    "    mean_abs_shap = mean_abs_shap.flatten()\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„± (ì ˆëŒ€ê°’ ê¸°ì¤€ ì •ë ¬)\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap_signed': mean_shap_signed,\n",
    "    'shap_abs': mean_abs_shap\n",
    "}).sort_values('shap_abs', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 54ê°œë¥¼ 27ê°œì”© ë‚˜ëˆ„ê¸°\n",
    "n_features = len(importance_df)\n",
    "half = n_features // 2  # 27ê°œ\n",
    "\n",
    "top_half = importance_df.iloc[:half].sort_values('shap_abs', ascending=True)  # ìƒìœ„ 27ê°œ\n",
    "bottom_half = importance_df.iloc[half:].sort_values('shap_abs', ascending=True)  # í•˜ìœ„ 27ê°œ\n",
    "\n",
    "# íˆ¬ì»¬ëŸ¼ ê·¸ë˜í”„ ìƒì„±\n",
    "fig, axes = plt.subplots(1, 2, figsize=(26, 14))\n",
    "\n",
    "for ax, data, title in zip(axes, [top_half, bottom_half], \n",
    "                            ['Top 27 Features (Rank 1-27)', 'Bottom 27 Features (Rank 28-54)']):\n",
    "    colors = ['#1f77b4' if x < 0 else '#d62728' for x in data['shap_signed']]\n",
    "    \n",
    "    ax.barh(range(len(data)), data['shap_signed'], color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=1.5)\n",
    "    \n",
    "    ax.set_yticks(range(len(data)))\n",
    "    ax.set_yticklabels(data['feature'], fontsize=9)\n",
    "    ax.set_xlabel('Mean SHAP value', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # ê°’ í‘œì‹œ (offsetì„ ë™ì ìœ¼ë¡œ ê³„ì‚°)\n",
    "    x_range = data['shap_abs'].max()\n",
    "    offset = x_range * 0.01  # ë§‰ëŒ€ ìµœëŒ€ê°’ì˜ 1%ë§Œí¼ offset\n",
    "    \n",
    "    for i, (feat, val_signed, val_abs) in enumerate(zip(data['feature'], data['shap_signed'], data['shap_abs'])):\n",
    "        sign = '+' if val_signed > 0 else ''\n",
    "        if val_signed < 0:\n",
    "            ax.text(val_signed - offset, i, f'{val_signed:.4f}', va='center', ha='right', fontsize=7)\n",
    "        else:\n",
    "            ax.text(val_signed + offset, i, f'{sign}{val_signed:.4f}', va='center', ha='left', fontsize=7)\n",
    "    \n",
    "    # ë²”ë¡€ (ê° ì„œë¸Œí”Œë¡¯ì— ë°°ì¹˜)\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#1f77b4', alpha=0.7, label='Negative: Legitimate'),\n",
    "        Patch(facecolor='#d62728', alpha=0.7, label='Positive: Phishing')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
    "\n",
    "plt.suptitle('SHAP Feature Importance with Direction (All 54 Features)', fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'shap_bar_twocolumn.png'), dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… Saved: {os.path.join(OUTPUT_DIR, 'shap_bar_twocolumn.png')}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š í”¼ì²˜ ìˆœìœ„ (ì ˆëŒ€ê°’ ê¸°ì¤€):\")\n",
    "print(\"-\"*60)\n",
    "for rank, (_, row) in enumerate(importance_df.iterrows(), 1):\n",
    "    direction = 'â†’ Phishing' if row['shap_signed'] > 0 else 'â† Legitimate'\n",
    "    print(f\"{rank:>2}. {row['feature']:<25} {row['shap_signed']:>10.6f} {direction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 12-3) SHAP Summary Plot (ë…¼ë¬¸ìš© íˆ¬ì»¬ëŸ¼) =====\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"/home/yu_mcc/QR_Phishing/phishing\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SHAP Summary Plot (54 Features - ë…¼ë¬¸ìš© íˆ¬ì»¬ëŸ¼)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# SHAP ê°’ shape í™•ì¸ ë° 2Dë¡œ ë³€í™˜\n",
    "print(f\"ì›ë³¸ SHAP values shape: {shap_values.shape}\")\n",
    "\n",
    "if len(shap_values.shape) == 3:\n",
    "    shap_values_2d = shap_values.squeeze()\n",
    "    print(f\"Squeezed SHAP values shape: {shap_values_2d.shape}\")\n",
    "else:\n",
    "    shap_values_2d = shap_values\n",
    "\n",
    "# í”¼ì²˜ ì¤‘ìš”ë„ ìˆœì„œë¡œ ì •ë ¬\n",
    "mean_abs_shap = np.mean(np.abs(shap_values_2d), axis=0).flatten()\n",
    "sorted_idx = np.argsort(mean_abs_shap)[::-1]  # ì¤‘ìš”ë„ ë†’ì€ ìˆœ\n",
    "\n",
    "# 54ê°œë¥¼ 27ê°œì”© ë‚˜ëˆ„ê¸°\n",
    "n_features = len(feature_names)\n",
    "half = n_features // 2  # 27ê°œ\n",
    "\n",
    "top_idx = sorted_idx[:half]  # ìƒìœ„ 27ê°œ\n",
    "bottom_idx = sorted_idx[half:]  # í•˜ìœ„ 27ê°œ\n",
    "\n",
    "# íˆ¬ì»¬ëŸ¼ ê·¸ë˜í”„ ìƒì„±\n",
    "fig, axes = plt.subplots(1, 2, figsize=(26, 14))\n",
    "\n",
    "for ax, indices, title in zip(axes, [top_idx, bottom_idx], \n",
    "                               ['Top 27 Features (Rank 1-27)', 'Bottom 27 Features (Rank 28-54)']):\n",
    "    # í•´ë‹¹ í”¼ì²˜ë§Œ ì¶”ì¶œ\n",
    "    shap_subset = shap_values_2d[:, indices]\n",
    "    X_subset = X_test_sample[:, indices]\n",
    "    feature_subset = [feature_names[i] for i in indices]\n",
    "    \n",
    "    plt.sca(ax)\n",
    "    shap.summary_plot(\n",
    "        shap_subset, \n",
    "        X_subset, \n",
    "        feature_names=feature_subset,\n",
    "        max_display=27,\n",
    "        show=False,\n",
    "        plot_size=None\n",
    "    )\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('SHAP Summary Plot (All 54 Features)', fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# ì €ì¥\n",
    "dot_plot_path = os.path.join(OUTPUT_DIR, 'shap_summary_twocolumn.png')\n",
    "plt.savefig(dot_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… Saved: {dot_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Top 10 í”¼ì²˜ ì¶œë ¥\n",
    "print(f\"\\nğŸ“Š Top 10 ì¤‘ìš” í”¼ì²˜:\")\n",
    "for i, idx in enumerate(sorted_idx[:10]):\n",
    "    print(f\"  {i+1}. {feature_names[idx]}: {mean_abs_shap[idx]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š í•´ì„ ê°€ì´ë“œ:\")\n",
    "print(\"=\"*70)\n",
    "print(\"  - ë¹¨ê°„ì (ë†’ì€ í”¼ì²˜ ê°’)ì´ ì˜¤ë¥¸ìª½(+)ì— ìœ„ì¹˜ â†’ ë†’ì€ ê°’ì´ í”¼ì‹± ì˜ˆì¸¡ ì¦ê°€\")\n",
    "print(\"  - íŒŒë€ì (ë‚®ì€ í”¼ì²˜ ê°’)ì´ ì™¼ìª½(-)ì— ìœ„ì¹˜ â†’ ë‚®ì€ ê°’ì´ ì •ìƒ ì˜ˆì¸¡ ì¦ê°€\")\n",
    "print(\"  - ì ë“¤ì˜ ë°€ë„ê°€ ë†’ì„ìˆ˜ë¡ í•´ë‹¹ SHAP ê°’ì— ë§ì€ ìƒ˜í”Œì´ ë¶„í¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 12-4) SHAP Horizontal Bar Chart (All 64 Features - With Sign) =====\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define OUTPUT_DIR if not already defined\n",
    "OUTPUT_DIR = \"/home/yu_mcc/QR_Phishing/phishing\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SHAP Horizontal Bar Chart (All 54 Features - With Sign)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Showing both negative and positive directions for feature importance\")\n",
    "print()\n",
    "\n",
    "# Check SHAP values shape\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"Feature names count: {len(feature_names)}\")\n",
    "\n",
    "# Calculate mean SHAP values with sign preserved (not absolute)\n",
    "mean_shap_signed = np.mean(shap_values, axis=0)  # Keep sign\n",
    "\n",
    "# Absolute values for importance magnitude\n",
    "mean_abs_shap = np.abs(mean_shap_signed)\n",
    "\n",
    "# Ensure 1D array\n",
    "if isinstance(mean_shap_signed, np.ndarray) and len(mean_shap_signed.shape) > 1:\n",
    "    mean_shap_signed = mean_shap_signed.flatten()\n",
    "    mean_abs_shap = mean_abs_shap.flatten()\n",
    "\n",
    "print(f\"Mean SHAP (signed) shape: {mean_shap_signed.shape}\")\n",
    "print(f\"Mean abs SHAP shape: {mean_abs_shap.shape}\")\n",
    "\n",
    "# Sort by absolute value and create dataframe\n",
    "if len(mean_shap_signed) == len(feature_names):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'shap_signed': mean_shap_signed,    # With sign\n",
    "        'shap_abs': mean_abs_shap            # Absolute value (importance)\n",
    "    }).sort_values('shap_abs', ascending=True)  # Sort by absolute value\n",
    "else:\n",
    "    print(f\"Warning: Feature count({len(feature_names)}) != SHAP value count({len(mean_shap_signed)})\")\n",
    "    print(\"Recalculating SHAP values...\")\n",
    "    mean_shap_signed = mean_shap_signed[:len(feature_names)]\n",
    "    mean_abs_shap = mean_abs_shap[:len(feature_names)]\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names[:len(mean_shap_signed)],\n",
    "        'shap_signed': mean_shap_signed,\n",
    "        'shap_abs': mean_abs_shap\n",
    "    }).sort_values('shap_abs', ascending=True)\n",
    "\n",
    "# Vertical bar chart (all 54 features with sign-based colors - rotated)\n",
    "fig, ax = plt.subplots(figsize=(30, 15))\n",
    "\n",
    "# Color by sign: Blue for negative, Red for positive\n",
    "colors = ['#1f77b4' if x < 0 else '#d62728' for x in importance_df['shap_signed']]\n",
    "\n",
    "# Sort by absolute value descending for better visualization\n",
    "importance_df_sorted = importance_df.sort_values('shap_abs', ascending=False)\n",
    "colors_sorted = ['#1f77b4' if x < 0 else '#d62728' for x in importance_df_sorted['shap_signed']]\n",
    "\n",
    "bars = ax.bar(range(len(importance_df_sorted)), importance_df_sorted['shap_signed'], \n",
    "              width=0.65, color=colors_sorted, alpha=0.7, edgecolor='black', linewidth=0.8)\n",
    "\n",
    "# Reference line at 0\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=1.5)\n",
    "\n",
    "ax.set_xticks(range(len(importance_df_sorted)))\n",
    "ax.set_xticklabels(importance_df_sorted['feature'], fontsize=10, rotation=45, ha='right')\n",
    "ax.margins(x=0.01)  # ì–‘ ë ê³µë°± ìµœì†Œí™”\n",
    "ax.set_ylabel('Mean SHAP value (Negative â† | â†’ Positive)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('SHAP Feature Importance with Direction (All 54 Features)\\nBlue=Legitimate Direction | Red=Phishing Direction', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Display values (sign and absolute value)\n",
    "for i, (feat, val_signed, val_abs) in enumerate(zip(importance_df_sorted['feature'], \n",
    "                                                     importance_df_sorted['shap_signed'],\n",
    "                                                     importance_df_sorted['shap_abs'])):\n",
    "    sign = '+' if val_signed > 0 else '-'\n",
    "    if val_signed < 0:\n",
    "        # ìŒìˆ˜: ë§‰ëŒ€ ì•„ë˜ì— ë°°ì¹˜\n",
    "        ax.text(i, val_signed - 0.0003, f'{sign}{val_abs:.4f}', \n",
    "               va='top', ha='center', fontsize=8, fontweight='bold')\n",
    "    else:\n",
    "        # ì–‘ìˆ˜: ë§‰ëŒ€ ìœ„ì— ë°°ì¹˜\n",
    "        ax.text(i, val_signed + 0.0003, f'{sign}{val_abs:.4f}', \n",
    "               va='bottom', ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#1f77b4', alpha=0.7, label='Negative (-): Increases Legitimate prediction'),\n",
    "    Patch(facecolor='#d62728', alpha=0.7, label='Positive (+): Increases Phishing prediction')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'shap_importance_all54_signed_vertical.png'), dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {os.path.join(OUTPUT_DIR, 'shap_importance_all54_signed_vertical.png')}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 64 Features by SHAP Importance (With Direction)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Rank':<5} {'Feature':<28} {'SHAP Signed Value':<15} {'Absolute Value':<15} {'Direction':<20}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for rank, (_, row) in enumerate(importance_df.sort_values('shap_abs', ascending=False).iterrows(), 1):\n",
    "    sign = 'NEGATIVE' if row['shap_signed'] < 0 else 'POSITIVE'\n",
    "    bar_len = int(abs(row['shap_signed']) * 100 / importance_df['shap_abs'].max())\n",
    "    bar = 'â–ˆ' * bar_len + 'â–‘' * (25 - bar_len)\n",
    "    direction = 'Legitimate Direction <-' if row['shap_signed'] < 0 else '-> Phishing Direction'\n",
    "    print(f\"{rank:<5} {row['feature']:<28} {row['shap_signed']:>13.6f}  {row['shap_abs']:>13.6f}  {direction:<20}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Negative (-): Higher values of this feature increase legitimate prediction (class 0)\")\n",
    "print(\"  - Positive (+): Higher values of this feature increase phishing prediction (class 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 12-6) íŠ¹ì´ í”¼ì²˜ ë¶„ì„: nb_star =====\n",
    "# print(\"=\"*70)\n",
    "# print(\"ğŸ” íŠ¹ì´ í”¼ì²˜ ë¶„ì„: nb_star\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # ì›ë³¸ ë°ì´í„°ì—ì„œ nb_star ë¶„ì„\n",
    "# nb_star_col_idx = feature_names.index('nb_star') if 'nb_star' in feature_names else None\n",
    "\n",
    "# if nb_star_col_idx is not None:\n",
    "#     print(f\"\\nğŸ“Š nb_star í”¼ì²˜ ë¶„ì„ (ì „ì²´ {len(df_work)} ìƒ˜í”Œ):\")\n",
    "    \n",
    "#     # nb_star ê°’ì˜ ë¶„í¬\n",
    "#     print(f\"\\nnb_star ê°’ ë¶„í¬:\")\n",
    "#     nb_star_dist = df_work['nb_star'].value_counts().sort_index()\n",
    "#     for val, count in nb_star_dist.items():\n",
    "#         print(f\"  nb_star = {val}: {count}ê°œ ({count/len(df_work)*100:.2f}%)\")\n",
    "    \n",
    "#     # nb_star = 1ì¼ ë•Œ í”¼ì‹± ë¹„ìœ¨\n",
    "#     nb_star_1 = df_work[df_work['nb_star'] == 1]\n",
    "#     if len(nb_star_1) > 0:\n",
    "#         phishing_count = (nb_star_1['status'] == 1).sum()\n",
    "#         legitimate_count = (nb_star_1['status'] == 0).sum()\n",
    "#         phishing_ratio = phishing_count / len(nb_star_1) * 100\n",
    "        \n",
    "#         print(f\"\\nğŸ¯ nb_star = 1ì¼ ë•Œ (ì´ {len(nb_star_1)}ê°œ):\")\n",
    "#         print(f\"  - í”¼ì‹±: {phishing_count}ê°œ ({phishing_ratio:.1f}%)\")\n",
    "#         print(f\"  - ì •ìƒ: {legitimate_count}ê°œ ({100-phishing_ratio:.1f}%)\")\n",
    "    \n",
    "#     # ì „ì²´ ë°ì´í„°ì˜ í”¼ì‹± ë¹„ìœ¨\n",
    "#     total_phishing = (df_work['status'] == 1).sum()\n",
    "#     total_legitimate = (df_work['status'] == 0).sum()\n",
    "#     total_phishing_ratio = total_phishing / len(df_work) * 100\n",
    "    \n",
    "#     print(f\"\\nğŸ“ˆ ì „ì²´ ë°ì´í„°ì˜ í”¼ì‹± ë¹„ìœ¨:\")\n",
    "#     print(f\"  - í”¼ì‹±: {total_phishing}ê°œ ({total_phishing_ratio:.1f}%)\")\n",
    "#     print(f\"  - ì •ìƒ: {total_legitimate}ê°œ ({100-total_phishing_ratio:.1f}%)\")\n",
    "    \n",
    "#     # nb_star = 0ê³¼ 1 ë¹„êµ\n",
    "#     nb_star_0 = df_work[df_work['nb_star'] == 0]\n",
    "#     if len(nb_star_0) > 0:\n",
    "#         phishing_count_0 = (nb_star_0['status'] == 1).sum()\n",
    "#         phishing_ratio_0 = phishing_count_0 / len(nb_star_0) * 100\n",
    "        \n",
    "#         print(f\"\\nğŸ“Š nb_star ê°’ì— ë”°ë¥¸ í”¼ì‹± ë¹„ìœ¨ ë¹„êµ:\")\n",
    "#         print(f\"  - nb_star = 0: {phishing_ratio_0:.1f}% í”¼ì‹± ({len(nb_star_0)}ê°œ ìƒ˜í”Œ)\")\n",
    "#         if len(nb_star_1) > 0:\n",
    "#             print(f\"  - nb_star = 1: {phishing_ratio:.1f}% í”¼ì‹± ({len(nb_star_1)}ê°œ ìƒ˜í”Œ)\")\n",
    "#             print(f\"  - ì°¨ì´: {abs(phishing_ratio - phishing_ratio_0):.1f}%p\")\n",
    "    \n",
    "#     # í†µê³„ì  ìœ ì˜ë¯¸ì„±\n",
    "#     print(f\"\\nâš ï¸ ë°ì´í„° ì–‘ ë¶€ì¡± ì—¬ë¶€:\")\n",
    "#     print(f\"  - nb_star = 1ì¸ ìƒ˜í”Œì´ 7ê°œëŠ” ì „ì²´ì˜ {7/len(df_work)*100:.3f}%ë¡œ ê·¹íˆ ì ìŒ\")\n",
    "#     print(f\"  - ì´ëŸ° ê²½ìš° í†µê³„ì ìœ¼ë¡œ ì‹ ë¢°ì„±ì´ ë‚®ìŒ (í‘œë³¸ í¬ê¸° < 30)\")\n",
    "#     print(f\"  - ë”°ë¼ì„œ \\\"ìš°ì—°ì˜ ì¼ì¹˜\\\"ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ\")\n",
    "#     print(f\"\\nğŸ’¡ ê²°ë¡ :\")\n",
    "#     print(f\"  - nb_star í”¼ì²˜ëŠ” ëŒ€ë¶€ë¶„ì˜ URLì—ì„œ 0ì´ê±°ë‚˜ ì•„ì£¼ ë‚®ì€ ê°’\")\n",
    "#     print(f\"  - 1ì¸ ê²½ìš°ê°€ ë§¤ìš° ë“œë¬¼ì–´ì„œ (0.06%) í†µê³„ì  ì˜ë¯¸ ë¶€ì¡±\")\n",
    "#     print(f\"  - ëª¨ë¸ í•™ìŠµì—ëŠ” ê±°ì˜ ì˜í–¥ì„ ì£¼ì§€ ì•Šì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ\")\n",
    "# else:\n",
    "#     print(\"âš ï¸ nb_star í”¼ì²˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# # ===== 12-5) ì „ì²´ í”¼ì²˜ ë¶„ì‚° ë¶„ì„: ì œê±°í•´ì•¼ í•  í”¼ì²˜ ì°¾ê¸° =====\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"ğŸ” ì „ì²´ í”¼ì²˜ ë¶„ì„: ëª¨ë‘ 0ì¸ í”¼ì²˜ ë˜ëŠ” ë¶„ì‚° 0ì¸ í”¼ì²˜ ì°¾ê¸°\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# variance_info = []\n",
    "# for col in feature_names:\n",
    "#     col_data = df_work[col].values\n",
    "#     variance = np.var(col_data)\n",
    "#     unique_count = len(np.unique(col_data))\n",
    "#     zero_count = (col_data == 0).sum()\n",
    "#     zero_ratio = zero_count / len(col_data) * 100\n",
    "    \n",
    "#     variance_info.append({\n",
    "#         'feature': col,\n",
    "#         'variance': variance,\n",
    "#         'unique_values': unique_count,\n",
    "#         'zero_count': zero_count,\n",
    "#         'zero_ratio': zero_ratio,\n",
    "#         'std': np.std(col_data)\n",
    "#     })\n",
    "\n",
    "# variance_df = pd.DataFrame(variance_info).sort_values('variance')\n",
    "\n",
    "# print(f\"\\nğŸ“Š í”¼ì²˜ë³„ ë¶„ì‚° (ë‚®ì€ ìˆœì„œ):\")\n",
    "# print(\"-\"*100)\n",
    "# print(f\"{'Rank':<5} {'Feature':<25} {'Variance':<12} {'Std Dev':<12} {'Unique':<8} {'Zero Count':<12} {'Zero %':<10}\")\n",
    "# print(\"-\"*100)\n",
    "\n",
    "# for rank, (_, row) in enumerate(variance_df.head(20).iterrows(), 1):\n",
    "#     marker = \"\"\n",
    "#     if row['variance'] == 0:\n",
    "#         marker = \" âš ï¸ ì œê±° ëŒ€ìƒ\"\n",
    "#     elif row['zero_ratio'] > 95:\n",
    "#         marker = \" âš ï¸ í¬ê·€ í”¼ì²˜\"\n",
    "    \n",
    "#     print(f\"{rank:<5} {row['feature']:<25} {row['variance']:<12.6f} {row['std']:<12.6f} {row['unique_values']:<8} {row['zero_count']:<12} {row['zero_ratio']:<10.2f}%{marker}\")\n",
    "\n",
    "# # ë¶„ì‚° 0 í”¼ì²˜ ì°¾ê¸°\n",
    "# zero_variance_features = variance_df[variance_df['variance'] == 0]['feature'].tolist()\n",
    "\n",
    "# if zero_variance_features:\n",
    "#     print(f\"\\nâŒ ë¶„ì‚°ì´ 0ì¸ í”¼ì²˜ (ë°˜ë“œì‹œ ì œê±°):\")\n",
    "#     for feat in zero_variance_features:\n",
    "#         print(f\"   - {feat}\")\n",
    "# else:\n",
    "#     print(f\"\\nâœ… ë¶„ì‚°ì´ 0ì¸ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# # ê±°ì˜ ëª¨ë‘ 0ì¸ í”¼ì²˜ ì°¾ê¸° (95% ì´ìƒ 0)\n",
    "# rare_features = variance_df[variance_df['zero_ratio'] > 95]['feature'].tolist()\n",
    "# if rare_features:\n",
    "#     print(f\"\\nâš ï¸ ê±°ì˜ ëª¨ë‘ 0ì¸ í”¼ì²˜ (95% ì´ìƒ 0):\")\n",
    "#     for feat in rare_features:\n",
    "#         row = variance_df[variance_df['feature'] == feat].iloc[0]\n",
    "#         print(f\"   - {feat}: {row['zero_count']}ê°œ 0 ({row['zero_ratio']:.2f}%)\")\n",
    "# else:\n",
    "#     print(f\"\\nâœ… ê±°ì˜ ëª¨ë‘ 0ì¸ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 13) ëª¨ë¸ ì €ì¥ (Keras ëª¨ë¸ + Scaler) =====\n",
    "# # â­ Chaquopyë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”í‹€ë¦°ì—ì„œ ì§ì ‘ ì¼€ë¼ìŠ¤ ëª¨ë¸ ë¡œë“œ\n",
    "\n",
    "# import os\n",
    "# import gc\n",
    "# import json\n",
    "\n",
    "# # ë©”ëª¨ë¦¬ ì •ë¦¬ (í¬ë˜ì‹œ ë°©ì§€)\n",
    "# gc.collect()\n",
    "# keras.backend.clear_session()\n",
    "\n",
    "# # ğŸ”§ Linux í™˜ê²½ ê²½ë¡œ\n",
    "# OUTPUT_DIR = \"/home/yu_mcc/QR_Phishing/phishing\"\n",
    "# CLASSIFIER_PATH = os.path.join(OUTPUT_DIR, \"classifier_model.keras\")  # .keras í˜•ì‹ ê¶Œì¥\n",
    "# SCALER_PATH = os.path.join(OUTPUT_DIR, \"scaler_params.json\")\n",
    "\n",
    "# # 1) Keras ëª¨ë¸ ì €ì¥ (.keras í˜•ì‹ - TF 2.15 ê¶Œì¥)\n",
    "# print(f\"Classifier ëª¨ë¸ ì €ì¥ ì¤‘: {CLASSIFIER_PATH}\")\n",
    "# classifier.save(CLASSIFIER_PATH)\n",
    "# print(\"âœ… Keras ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "# print(f\"   í¬ê¸°: {os.path.getsize(CLASSIFIER_PATH) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# # 2) Scaler íŒŒë¼ë¯¸í„° ì €ì¥ (Android ì „ì²˜ë¦¬ìš©)\n",
    "# print(f\"\\nScaler íŒŒë¼ë¯¸í„° ì €ì¥ ì¤‘: {SCALER_PATH}\")\n",
    "# with open(SCALER_PATH, 'w') as f:\n",
    "#     json.dump(scaler_params, f, indent=2)\n",
    "# print(\"âœ… Scaler íŒŒë¼ë¯¸í„° ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ (Chaquopyìš© ì¤€ë¹„ë¨)\")\n",
    "# print(\"=\" * 60)\n",
    "# print(\"\\nğŸ“± Android Chaquopy ì‚¬ìš©ë²•:\")\n",
    "# print(\"   1. build.gradleì— Chaquopy ì¶”ê°€\")\n",
    "# print(\"   2. classifier_model.kerasë¥¼ assetsì— ë³µì‚¬\")\n",
    "# print(\"   3. scaler_params.jsonì„ assetsì— ë³µì‚¬\")\n",
    "# print(\"   4. Python ëª¨ë“ˆì—ì„œ ëª¨ë¸ ë¡œë“œ í›„ ì¶”ë¡ \")\n",
    "# print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ TFLite ë³€í™˜ ë¬¸ì œ í•´ê²°: from_keras_model ì§ì ‘ ë³€í™˜\n",
    "\n",
    "**ë¬¸ì œ ì›ì¸:**\n",
    "- `classifier.export()` â†’ `from_saved_model()` ë°©ì‹ì€ Keras 3ì—ì„œ BatchNormalizationì˜ `moving_mean/variance`ê°€ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ\n",
    "- TFLite ì¶œë ¥ì´ ~0.5ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì€ BatchNormì´ \"í•™ìŠµ ëª¨ë“œ\"ì²˜ëŸ¼ ë™ì‘í•˜ê±°ë‚˜ í†µê³„ê°€ ì†ì‹¤ëœ ì¦ê±°\n",
    "\n",
    "**í•´ê²°ì±…:**\n",
    "- `tf.lite.TFLiteConverter.from_keras_model(classifier)`ë¡œ ì§ì ‘ ë³€í™˜\n",
    "- SavedModel ì¤‘ê°„ ë‹¨ê³„ë¥¼ ê±°ì¹˜ì§€ ì•Šì•„ BatchNorm í†µê³„ê°€ ì •í™•íˆ ìœ ì§€ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 14) TFLite ëª¨ë¸ í…ŒìŠ¤íŠ¸ =====\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # â­ ì¤‘ìš”: TFLite íŒŒì¼ì„ ìƒˆë¡œ ë¡œë“œ (ì´ì „ interpreter ì¬ì‚¬ìš© ë°©ì§€)\n",
    "# TFLITE_PATH = \"/Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/phishing_classifier.tflite\"\n",
    "\n",
    "# print(f\"TFLite ëª¨ë¸ ë¡œë“œ: {TFLITE_PATH}\")\n",
    "# interpreter = tf.lite.Interpreter(model_path=TFLITE_PATH)\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# input_details = interpreter.get_input_details()\n",
    "# output_details = interpreter.get_output_details()\n",
    "\n",
    "# print(f\"ì…ë ¥ shape: {input_details[0]['shape']}\")\n",
    "# print(f\"ì…ë ¥ dtype: {input_details[0]['dtype']}\")\n",
    "# print(f\"ì¶œë ¥ shape: {output_details[0]['shape']}\")\n",
    "\n",
    "# # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì¤€ë¹„\n",
    "# test_sample = X_test_scaled[0:20].astype(np.float32)\n",
    "# batch_size = test_sample.shape[0]\n",
    "\n",
    "# # TFLiteëŠ” ë³´í†µ [1, input_dim] í˜•íƒœì´ë¯€ë¡œ ìƒ˜í”Œë³„ë¡œ ì¶”ë¡ \n",
    "# print(f\"\\nìƒ˜í”Œ {batch_size}ê°œì— ëŒ€í•´ ì¶”ë¡  ìˆ˜í–‰...\")\n",
    "\n",
    "# tflite_outputs = []\n",
    "# for i in range(batch_size):\n",
    "#     sample = test_sample[i:i+1].astype(np.float32)\n",
    "#     interpreter.set_tensor(input_details[0]['index'], sample)\n",
    "#     interpreter.invoke()\n",
    "#     output = interpreter.get_tensor(output_details[0]['index'])\n",
    "#     tflite_outputs.append(output[0][0])\n",
    "\n",
    "# tflite_output = np.array(tflite_outputs).reshape(-1, 1)\n",
    "\n",
    "# # â­ ë³€í™˜ì— ì¼ë˜ classifier ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë©”ëª¨ë¦¬ ëª¨ë¸ë¡œ ì •í™•í•œ ë¹„êµ)\n",
    "# keras_output = classifier.predict(test_sample, verbose=0)\n",
    "\n",
    "# # ê²°ê³¼ ë¹„êµ\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"===== TFLite vs Keras ì¶œë ¥ ë¹„êµ =====\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# for i in range(batch_size):\n",
    "#     actual = \"ì •ìƒ\" if y_test[i] == 1 else \"í”¼ì‹±\"\n",
    "#     diff = abs(tflite_output[i][0] - keras_output[i][0])\n",
    "#     match = \"âœ…\" if diff < 0.01 else \"âŒ\"\n",
    "#     print(f\"ìƒ˜í”Œ {i+1}: TFLite={tflite_output[i][0]:.6f}, Keras={keras_output[i][0]:.6f}, ì°¨ì´={diff:.6f} {match}, ì‹¤ì œ={actual}\")\n",
    "\n",
    "# # í†µê³„\n",
    "# mae = np.mean(np.abs(tflite_output - keras_output))\n",
    "# max_err = np.max(np.abs(tflite_output - keras_output))\n",
    "# print(f\"\\nğŸ“Š í†µê³„:\")\n",
    "# print(f\"   í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE): {mae:.6f}\")\n",
    "# print(f\"   ìµœëŒ€ ì˜¤ì°¨: {max_err:.6f}\")\n",
    "\n",
    "# if max_err < 0.001:\n",
    "#     print(\"\\nğŸ‰ TFLiteì™€ Keras ì¶œë ¥ì´ ê±°ì˜ ë™ì¼í•©ë‹ˆë‹¤! (ì˜¤ì°¨ < 0.001)\")\n",
    "# elif max_err < 0.01:\n",
    "#     print(\"\\nâœ… TFLiteì™€ Keras ì¶œë ¥ì´ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤. (ì˜¤ì°¨ < 0.01)\")\n",
    "# else:\n",
    "#     print(f\"\\nâš ï¸ TFLiteì™€ Keras ì¶œë ¥ì— ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. (ìµœëŒ€ ì˜¤ì°¨: {max_err:.4f})\")\n",
    "#     print(\"   â†’ ëª¨ë¸ì„ ë‹¤ì‹œ ë³€í™˜í•˜ê±°ë‚˜, ì»¤ë„ì„ ì¬ì‹œì‘ í›„ ì²˜ìŒë¶€í„° ì‹¤í–‰í•´ë³´ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15) TFLite ë³€í™˜ (Kotlin í˜¸í™˜ì„±)\n",
    "\n",
    "**Kotlinì—ì„œì˜ ëª¨ë¸ ì§€ì›:**\n",
    "- âŒ SavedModel (.pb) - ì§€ì› ì•ˆ í•¨\n",
    "- âœ… TFLite (.tflite) - ê³µì‹ ì§€ì›\n",
    "- âš ï¸ Keras (.keras) - Chaquopyë§Œ ì§€ì›\n",
    "\n",
    "ë”°ë¼ì„œ ì˜¨-ë””ë°”ì´ìŠ¤ ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•´ **TFLite ë³€í™˜**ì´ í•„ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "**ë³€í™˜ ë°©ì‹:**\n",
    "- `from_keras_model()` - Keras ëª¨ë¸ â†’ TFLite (ê¶Œì¥)\n",
    "- BatchNormalization í†µê³„ ì •í™•íˆ ìœ ì§€\n",
    "- ì–‘ìí™” ì˜µì…˜ìœ¼ë¡œ í¬ê¸° ìµœì í™” ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 15) TFLite ë³€í™˜ (from_keras_model) =====\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# TFLite ì €ì¥ ê²½ë¡œ\n",
    "TFLITE_PATH = os.path.join(OUTPUT_DIR, \"phishing_classifier.tflite\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”„ TFLite ë³€í™˜ ì‹œì‘ (from_keras_model)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# â­ ì¤‘ìš”: from_keras_model() ì§ì ‘ ë³€í™˜ (SavedModel ì¤‘ê°„ ë‹¨ê³„ ì—†ìŒ)\n",
    "# ì´ë ‡ê²Œ í•´ì•¼ BatchNormalization í†µê³„ê°€ ì •í™•íˆ ìœ ì§€ë¨\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(classifier)\n",
    "\n",
    "# ìµœì í™” ì˜µì…˜ (ì„ íƒì‚¬í•­)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]  # ì–‘ìí™” ì˜µì…˜\n",
    "# converter.target_spec.supported_ops = [\n",
    "#     tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "#     tf.lite.OpsSet.SELECT_TF_OPS\n",
    "# ]\n",
    "\n",
    "# ë³€í™˜ ìˆ˜í–‰\n",
    "print(\"ë³€í™˜ ì¤‘ (ì´ ê³¼ì •ì€ 2-3ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)...\")\n",
    "try:\n",
    "    tflite_model = converter.convert()\n",
    "    print(\"âœ… ë³€í™˜ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}\")\n",
    "    raise\n",
    "\n",
    "# TFLite íŒŒì¼ ì €ì¥\n",
    "with open(TFLITE_PATH, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "file_size_mb = os.path.getsize(TFLITE_PATH) / 1024 / 1024\n",
    "print(f\"âœ… TFLite íŒŒì¼ ì €ì¥ ì™„ë£Œ: {TFLITE_PATH}\")\n",
    "print(f\"   íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ëª¨ë¸ ë¹„êµ\")\n",
    "print(\"=\"*60)\n",
    "keras_size = os.path.getsize(CLASSIFIER_PATH) / 1024 / 1024\n",
    "print(f\"Keras ëª¨ë¸: {keras_size:.2f} MB\")\n",
    "print(f\"TFLite ëª¨ë¸: {file_size_mb:.2f} MB\")\n",
    "print(f\"ì••ì¶•ë¥ : {(1 - file_size_mb/keras_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 16) TFLite ëª¨ë¸ ë¡œë“œ ë° ê¸°ë³¸ í…ŒìŠ¤íŠ¸ =====\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” TFLite ëª¨ë¸ ê²€ì¦\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# TFLite ì¸í„°í”„ë¦¬í„° ìƒì„±\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# ì…ë ¥/ì¶œë ¥ ì •ë³´ í™•ì¸\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(f\"\\nì…ë ¥ ì •ë³´:\")\n",
    "print(f\"  - Shape: {input_details[0]['shape']}\")\n",
    "print(f\"  - ë°ì´í„°íƒ€ì…: {input_details[0]['dtype']}\")\n",
    "print(f\"  - ì¸ë±ìŠ¤: {input_details[0]['index']}\")\n",
    "\n",
    "print(f\"\\nì¶œë ¥ ì •ë³´:\")\n",
    "print(f\"  - Shape: {output_details[0]['shape']}\")\n",
    "print(f\"  - ë°ì´í„°íƒ€ì…: {output_details[0]['dtype']}\")\n",
    "print(f\"  - ì¸ë±ìŠ¤: {output_details[0]['index']}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì¤€ë¹„ (10ê°œ)\n",
    "test_samples = X_test_scaled[:10].astype(np.float32)\n",
    "actual_labels = y_test[:10]\n",
    "\n",
    "print(f\"\\ní…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {test_samples.shape[0]}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 17) Keras vs TFLite ìƒì„¸ ë¹„êµ =====\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š Keras vs TFLite ì¶œë ¥ ë¹„êµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Keras ì˜ˆì¸¡\n",
    "print(\"\\n1ï¸âƒ£ Keras ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...\")\n",
    "keras_preds = classifier.predict(test_samples, verbose=0)\n",
    "print(f\"âœ… Keras ì˜ˆì¸¡ ì™„ë£Œ: shape={keras_preds.shape}\")\n",
    "\n",
    "# TFLite ì˜ˆì¸¡\n",
    "print(\"\\n2ï¸âƒ£ TFLite ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...\")\n",
    "tflite_preds = []\n",
    "\n",
    "for i in range(len(test_samples)):\n",
    "    sample = test_samples[i:i+1].astype(np.float32)\n",
    "    \n",
    "    # TFLite ì¸í„°í”„ë¦¬í„°ì— ì…ë ¥ ì„¤ì •\n",
    "    interpreter.set_tensor(input_details[0]['index'], sample)\n",
    "    \n",
    "    # ì¶”ë¡  ìˆ˜í–‰\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # ì¶œë ¥ ì¶”ì¶œ\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    tflite_preds.append(output.flatten()[0])\n",
    "\n",
    "tflite_preds = np.array(tflite_preds).reshape(-1, 1)\n",
    "print(f\"âœ… TFLite ì˜ˆì¸¡ ì™„ë£Œ: shape={tflite_preds.shape}\")\n",
    "\n",
    "# ìƒì„¸ ë¹„êµ í…Œì´ë¸”\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'ìƒ˜í”Œ':<6} {'ì‹¤ì œ':<8} {'Keras':<12} {'TFLite':<12} {'ì°¨ì´':<12} {'ì¼ì¹˜':<6}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "differences = []\n",
    "for i in range(len(test_samples)):\n",
    "    actual = \"ì •ìƒ\" if actual_labels[i] == 1 else \"í”¼ì‹±\"\n",
    "    keras_val = keras_preds[i][0]\n",
    "    tflite_val = tflite_preds[i][0]\n",
    "    diff = abs(keras_val - tflite_val)\n",
    "    differences.append(diff)\n",
    "    \n",
    "    match = \"âœ…\" if diff < 0.001 else (\"âš ï¸ \" if diff < 0.01 else \"âŒ\")\n",
    "    \n",
    "    print(f\"{i+1:<6} {actual:<8} {keras_val:<12.6f} {tflite_val:<12.6f} {diff:<12.6f} {match:<6}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# í†µê³„\n",
    "mae = np.mean(differences)\n",
    "max_err = np.max(differences)\n",
    "min_err = np.min(differences)\n",
    "std_err = np.std(differences)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ì˜¤ì°¨ í†µê³„:\")\n",
    "print(f\"  - í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE): {mae:.8f}\")\n",
    "print(f\"  - ìµœëŒ€ ì˜¤ì°¨: {max_err:.8f}\")\n",
    "print(f\"  - ìµœì†Œ ì˜¤ì°¨: {min_err:.8f}\")\n",
    "print(f\"  - í‘œì¤€í¸ì°¨: {std_err:.8f}\")\n",
    "\n",
    "# ê²€ì¦ ê²°ê³¼\n",
    "print(f\"\\nâœ… ê²€ì¦ ê²°ê³¼:\")\n",
    "if max_err < 0.0001:\n",
    "    print(f\"   ğŸ‰ ì™„ë²½í•œ ì¼ì¹˜! (ì˜¤ì°¨ < 0.0001)\")\n",
    "elif max_err < 0.001:\n",
    "    print(f\"   âœ… ê±°ì˜ ë™ì¼ (ì˜¤ì°¨ < 0.001)\")\n",
    "elif max_err < 0.01:\n",
    "    print(f\"   âš ï¸ ì•½ê°„ì˜ ì°¨ì´ (ì˜¤ì°¨ < 0.01)\")\n",
    "else:\n",
    "    print(f\"   âŒ í° ì°¨ì´ ë°œìƒ (ì˜¤ì°¨ > 0.01)\")\n",
    "    print(f\"   â†’ ëª¨ë¸ì„ ë‹¤ì‹œ ë³€í™˜í•˜ê±°ë‚˜ ì»¤ë„ì„ ì¬ì‹œì‘ í›„ ì²˜ìŒë¶€í„° ì‹¤í–‰í•´ë³´ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 18) ë¶„ë¥˜ ì •í™•ë„ ê²€ì¦ =====\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ ë¶„ë¥˜ ì •í™•ë„ ê²€ì¦\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ì„ê³„ê°’ ì„¤ì •\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# Keras ì´ì§„ ë¶„ë¥˜\n",
    "keras_binary = (keras_preds > THRESHOLD).astype(int).flatten()\n",
    "\n",
    "# TFLite ì´ì§„ ë¶„ë¥˜\n",
    "tflite_binary = (tflite_preds > THRESHOLD).astype(int).flatten()\n",
    "\n",
    "# ì‹¤ì œ ë ˆì´ë¸”\n",
    "actual_binary = actual_labels.astype(int)\n",
    "\n",
    "# ì •í™•ë„ ê³„ì‚°\n",
    "keras_acc = accuracy_score(actual_binary, keras_binary)\n",
    "tflite_acc = accuracy_score(actual_binary, tflite_binary)\n",
    "\n",
    "print(f\"\\nğŸ“Š ì •í™•ë„ ë¹„êµ (ì„ê³„ê°’: {THRESHOLD}):\")\n",
    "print(f\"  - Keras ì •í™•ë„: {keras_acc:.4f} ({sum(keras_binary == actual_binary)}/{len(actual_binary)})\")\n",
    "print(f\"  - TFLite ì •í™•ë„: {tflite_acc:.4f} ({sum(tflite_binary == actual_binary)}/{len(actual_binary)})\")\n",
    "print(f\"  - ì°¨ì´: {abs(keras_acc - tflite_acc):.6f}\")\n",
    "\n",
    "# AUC ê³„ì‚°\n",
    "keras_auc = roc_auc_score(actual_binary, keras_preds)\n",
    "tflite_auc = roc_auc_score(actual_binary, tflite_preds)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ AUC ë¹„êµ:\")\n",
    "print(f\"  - Keras AUC: {keras_auc:.4f}\")\n",
    "print(f\"  - TFLite AUC: {tflite_auc:.4f}\")\n",
    "print(f\"  - ì°¨ì´: {abs(keras_auc - tflite_auc):.6f}\")\n",
    "\n",
    "# ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "print(f\"\\nğŸ“‹ Keras ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(actual_binary, keras_binary, \n",
    "                            target_names=['Phishing(0)', 'Legitimate(1)']))\n",
    "\n",
    "print(f\"\\nğŸ“‹ TFLite ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(actual_binary, tflite_binary, \n",
    "                            target_names=['Phishing(0)', 'Legitimate(1)']))\n",
    "\n",
    "# ìµœì¢… ê²€ì¦\n",
    "if keras_acc == tflite_acc and abs(keras_auc - tflite_auc) < 0.0001:\n",
    "    print(\"\\nğŸ‰ ì™„ë²½í•œ ì¼ì¹˜! Kerasì™€ TFLiteì˜ ë¶„ë¥˜ ê²°ê³¼ê°€ ë™ì¼í•©ë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"\\nâœ… TFLite ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 19) Android assetsë¡œ TFLite ëª¨ë¸ ë³µì‚¬ =====\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# print(\"=\"*60)\n",
    "# print(\"ğŸ“± Android assetsë¡œ ëª¨ë¸ ë°°í¬\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# ANDROID_ASSETS = \"/home/yu_mcc/QR_Phishing/app/src/main/assets\"\n",
    "\n",
    "# # TFLite ëª¨ë¸ ë³µì‚¬\n",
    "# try:\n",
    "#     shutil.copy(TFLITE_PATH, os.path.join(ANDROID_ASSETS, \"phishing_classifier.tflite\"))\n",
    "#     print(f\"âœ… TFLite ëª¨ë¸ ë³µì‚¬ ì™„ë£Œ\")\n",
    "#     print(f\"   {os.path.join(ANDROID_ASSETS, 'phishing_classifier.tflite')}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âš ï¸ TFLite ë³µì‚¬ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# # feature_info.jsonë„ ë³µì‚¬ (í”¼ì²˜ ìˆœì„œ ì •ì˜ìš©)\n",
    "# feature_info_path = os.path.join(OUTPUT_DIR, \"feature_info.json\")\n",
    "# if os.path.exists(feature_info_path):\n",
    "#     shutil.copy(feature_info_path, os.path.join(ANDROID_ASSETS, \"feature_info.json\"))\n",
    "#     print(f\"âœ… feature_info.json ë³µì‚¬ ì™„ë£Œ\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"âœ… ìµœì¢… ë°°í¬ ì™„ë£Œ\")\n",
    "# print(\"=\"*60)\n",
    "# print(\"\\nğŸ“ Android assets íŒŒì¼:\")\n",
    "# for file in os.listdir(ANDROID_ASSETS):\n",
    "#     if file.endswith(('.tflite', '.keras', '.json')):\n",
    "#         full_path = os.path.join(ANDROID_ASSETS, file)\n",
    "#         size_kb = os.path.getsize(full_path) / 1024\n",
    "#         print(f\"  - {file:<30} ({size_kb:>8.1f} KB)\")\n",
    "\n",
    "# print(\"\\nğŸ“± Kotlinì—ì„œ TFLite ë¡œë“œ ë°©ë²•:\")\n",
    "# print(\"   1. AssetManagerë¡œ assetsì—ì„œ íŒŒì¼ ë¡œë“œ\")\n",
    "# print(\"   2. tf.lite.Interpreter(model_buffer) ìƒì„±\")\n",
    "# print(\"   3. interpreter.allocate_tensors()\")\n",
    "# print(\"   4. ì „ì²˜ë¦¬ëœ ì…ë ¥ìœ¼ë¡œ ì¶”ë¡  ìˆ˜í–‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}